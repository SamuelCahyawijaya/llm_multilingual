{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d20f5b-de75-47c7-bfcf-e3873ab93bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re                \n",
    "import json\n",
    "import gdown\n",
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "alphaonly = re.compile('[^a-zA-Z ?]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "de5de2e7-07e0-4de7-9944-2a0bbda55b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spart_qa(num_dataset=30, output_gold=True, exp_type='1-reasoning-type'):\n",
    "    \"\"\"\n",
    "        SpaRTQA\n",
    "        SpartQA is a textual question answering benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior datasets and that is challenging for state-of-the-art language models (LM). SPARTQA is built on NLVR’s images containing more objects with richer spatial structures. SPARTQA’s stories are more natural, have more sentences, and richer in spatial relations in each sentence, and the questions require deeper reasoning and have four types: find relation (FR), find blocks (FB), choose object (CO), and yes/no (YN), which allows for more fine-grained analysis of models’ capabilities\n",
    "        \n",
    "        {\n",
    "            \"data_source\": [\"https://github.com/HLR/SpartQA_generation\",\n",
    "                \"https://drive.google.com/file/d/12s2olGDV0ruywPtLhGL5M-1e4CbQrA8k/view\"],\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"spatial-reasoning\",\n",
    "            \"answer-type\": \"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain score 1 (for True) or 0 (for False), based on gold labels from original data. The average score serves as overall accuracy. \\\",\n",
    "            \"note\": \"Because each context has several corresponding questions for each type of reasoning, we select the first sample for each type and add it to our test set. That means, each context will usually have 4 questions in our test set. Also, we take the train split because it provides image to double check the gold answer.\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    data = open('data/human_test.json', 'r').read()\n",
    "    data = data.replace('false', 'False').replace('true', 'True')\n",
    "    data = eval(data)['data']\n",
    "    test_examples, test_golds, test_ids, candidate_choices = [], [], [], []\n",
    "\n",
    "    if exp_type == '1-reasoning-type':\n",
    "        num_context = num_dataset\n",
    "        for i in range(num_context):\n",
    "            story = data[i]['story']\n",
    "            questions = data[i]['questions']\n",
    "            for j, q in enumerate(questions):\n",
    "                if not len(q['reasoning_type']) == 1:\n",
    "                    continue\n",
    "\n",
    "                q_type = q['q_type']\n",
    "                if (q_type not in ['FR', 'YN', 'CO', 'FB']):\n",
    "                    continue\n",
    "\n",
    "                if 'candidate_answers' in q.keys():\n",
    "                    candidate_str = ', '.join([f'{k}. {t}' for k, t in enumerate(q['candidate_answers'])])\n",
    "                    candidate_str = candidate_str.replace('DK', \"don't know\")\n",
    "                    candidate_choices.append([c.strip() for c in q['candidate_answers']])\n",
    "                    if len(candidate_choices[-1]) == 0:\n",
    "                        candidate_choices[-1] = ['Yes', 'No']\n",
    "                    elif candidate_choices[-1][0] != 'A':\n",
    "                        candidate_choices[-1] = [str(i) for i in range(len(candidate_choices[-1]))]\n",
    "                else:\n",
    "                    candidate_str = ''\n",
    "                    candidate_choices.append(['Yes', 'No'])\n",
    "\n",
    "                test_examples.append(f'Given the description: {story[0]}. {q[\"question\"]} {candidate_str}')\n",
    "                test_golds.append([str(a) for a in q['answer']])\n",
    "                test_ids.append(f'{i}|{j}')\n",
    "\n",
    "    elif exp_type == '2-reasoning-type':\n",
    "        count_by_q_type = {x: 0 for x in ['FR', 'YN', 'CO', 'FB']}\n",
    "        num_samples_each_q_type = (num_dataset-1)//4+1\n",
    "        num_samples_taken = 0\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            instance = data[i]\n",
    "            story = instance['story']\n",
    "            questions = instance['questions']\n",
    "            question_type_taken = []\n",
    "\n",
    "            for j, q in enumerate(questions):\n",
    "                if not len(q['reasoning_type']) == 2:\n",
    "                    continue\n",
    "\n",
    "                q_type = q['q_type']\n",
    "                if (q_type not in ['FR', 'YN', 'CO', 'FB']):\n",
    "                    continue\n",
    "                else:\n",
    "                    test_ids.append(f'{i}|{j}')\n",
    "                    question_type_taken.append(q_type)\n",
    "                    count_by_q_type[q_type] += 1\n",
    "                    num_samples_taken += 1\n",
    "\n",
    "                if 'candidate_answers' in q.keys():\n",
    "                    candidate_str = ', '.join([f'{k}. {t}' for k, t in enumerate(q['candidate_answers'])])\n",
    "                    candidate_str = candidate_str.replace('DK', \"don't know\")\n",
    "                    candidate_choices.append([c.strip() for c in q['candidate_answers']])\n",
    "                    if len(candidate_choices[-1]) == 0:\n",
    "                        candidate_choices[-1] = ['Yes', 'No']\n",
    "                    elif candidate_choices[-1][0] != 'A':\n",
    "                        candidate_choices[-1] = [str(i) for i in range(len(candidate_choices[-1]))]\n",
    "                else:\n",
    "                    candidate_str = ''\n",
    "                    candidate_choices.append(['Yes', 'No'])\n",
    "                test_examples.append(f'Given the description: {story[0]}. {q[\"question\"]} {candidate_str}')\n",
    "                test_golds.append([str(a) for a in q['answer']])\n",
    "\n",
    "    if output_gold:\n",
    "        return test_examples, candidate_choices, test_ids, test_golds,\n",
    "    else:\n",
    "        return test_examples, candidate_choices, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c1ea3ed6-76a9-4325-8b72-375cae51f824",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choice', 'label'],\n",
       "    num_rows: 261\n",
       "})"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries, choices, ids, labels = spart_qa(num_dataset=49, output_gold=True, exp_type='1-reasoning-type')\n",
    "Dataset.from_pandas(pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "b7835801-ec8b-424a-848a-b1bec647fbc2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'choice', 'label'],\n",
       "    num_rows: 217\n",
       "})"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries, choices, ids, labels = spart_qa(num_dataset=49, output_gold=True, exp_type='2-reasoning-type')\n",
    "Dataset.from_pandas(pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e771be6e-3e63-45bf-b5cb-c370e324b767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def math():\n",
    "#     \"\"\"\n",
    "#         Math dataset\n",
    "#         This dataset contains mathematical question and answer pairs, from a range of question types at roughly school-level difficulty. This is designed to test the mathematical learning and algebraic reasoning skills of learning models.\n",
    "    \n",
    "#         {\n",
    "#             \"data_source\": [\"https://console.cloud.google.com/storage/browser/_details/mathematics-dataset/mathematics_dataset-v1.0.tar.gz\",\n",
    "#                 \"https://huggingface.co/datasets/math_dataset/viewer/algebra__linear_1d/test\"],\n",
    "#             \"license\": \"Apache License 2.0\",\n",
    "#             \"evaluation-method\": \"human-evaluation\",\n",
    "#             \"evaluation-aspect\": \"mathematical-reasoning\",\n",
    "#             \"answer-type\": \"\",\n",
    "#             \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "#                                 The generated answer is evaluated by a human to obtain score 1 (for True) or 0 (for False), based on gold labels from original data. The average score serves as overall accuracy. \\\",\n",
    "#             \"note\": \"Since the original dataset is 2.2GB, and the license Apache License 2.0 allows to modify the dataset and redistribute it, we build the set to test ChatGPT by copying from dataviewer of https://huggingface.co/datasets/math_dataset. In specific, we pick the first 5 samples from 6 categories: 'algebra__linear_1d', 'arithmetic__add_or_sub', 'calculus__differentiate', 'comparison__closest', 'measurement__conversion', 'numbers__base_conversion'. The set can be found in ./data/math_deepmind_30_samples.csv\".\n",
    "#         }\n",
    "#     \"\"\"\n",
    "#     data = pd.read_csv('data/math_deepmind_30_samples.csv')\n",
    "#     test_ids = list(range(5))*6\n",
    "#     test_examples, test_golds = data['Question'].tolist(), data['Gold_answer'].tolist()\n",
    "    \n",
    "#     return test_examples, test_ids, test_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49a99c26-9658-4820-a5c5-8d1122c3dda2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'question', 'label'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# queries, ids, labels = math()\n",
    "# queries = queries[:14] + queries[15:]\n",
    "# labels = labels[:14] + labels[15:]\n",
    "\n",
    "# Dataset.from_dict({\n",
    "#     'id': ids,\n",
    "#     'question': queries,\n",
    "#     'label': labels\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "c9f8c78f-fe2b-4eea-8843-0fcc55ab9f9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def timedial(num_dataset=30):\n",
    "    \"\"\"\n",
    "        TimeDial\n",
    "        TimeDial presents a crowdsourced English challenge set, for temporal commonsense reasoning, formulated as a multiple choice cloze task with around 1.5k carefully curated dialogs. The dataset is derived from the DailyDialog (Li et al., 2017), which is a multi-turn dialog corpus. We follow the format of the task in the BIG-Bench benchmark, which is multiple-choice (single correct answer). Note that the correct answer should be 0 or 1, say if answer of ChatGPT indicates the answer as 0 or 1, we mark the answer as True.\n",
    "\n",
    "        {\n",
    "            \"data_source\": \"https://github.com/google-research-datasets/TimeDial/blob/main/test.json\",\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"temporal-reasoning\",\n",
    "            \"answer-type\": \"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain score 1 (for True) or 0 (for False), based on gold labels from original data. The average score serves as overall accuracy. \\\",\n",
    "        }\n",
    "    \"\"\"\n",
    "    timedial_data = json.load(open('data/timedial_test.json', 'r'))\n",
    "    test_examples = []\n",
    "    test_ids = list(range(num_dataset))\n",
    "    test_golds = ['0 or 1 (if option 1 != none)']*num_dataset\n",
    "\n",
    "    for ex in timedial_data[:num_dataset]:\n",
    "        conversation = ex['conversation']\n",
    "        choices = (ex['correct1'], ex['correct2'], ex['incorrect1'], ex['incorrect2']) # ex['correct2'] if ex['correct2'] != 'none' else ex['correct1']\n",
    "\n",
    "        conversation = 'Given the conversation:\\n' + '\\n'.join(conversation) + '\\n'\n",
    "        choices = 'Candidate choices to fill in the <mask>: ' + ' '.join([f'{j}. {t},' for j, t in enumerate(choices)]) + '\\n'\n",
    "        caution = 'Note that there may not be enough information to certainly fill in the <mask>, but from commonsense reasoning, you can surely narrow down what are the most probable choices to fill in the <mask>. Please select the most propable choice from candidates and explain your choice.'\n",
    "        final = conversation+choices+caution\n",
    "        test_examples.append(final.replace('\"', \"'\"))\n",
    "\n",
    "    return test_examples, [[0,1,2,3] for i in range(len(test_ids))], test_ids, test_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0301459e-cc39-4ee0-a63e-66cf0385cad8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries, choices, ids, labels = timedial(num_dataset=1446)\n",
    "for i, q in enumerate(queries):\n",
    "    if '1. none' in q:\n",
    "        labels[i] = [0,1]\n",
    "    else:\n",
    "        labels[i] = [0]\n",
    "        \n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "3884a115-fe24-471a-846c-a2726c064cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pep_3k(num_dataset=30, output_gold=True):\n",
    "    \"\"\"\n",
    "        Pep-3k\n",
    "        Pep-3k is a dataset of physical semantic plausibility judgments of single events. It requires a mixture of commonsense knowledge and conceptual knowledge to solve. Each event consists of a subject, a verb, and an object, i.e it has the simple s-v-o format. For example, the event can be man swallow paintball, with the label 0 (implausible). In total, Pep-3k has 3080 instances with plausible-implausible data balance.\n",
    "\n",
    "        {\n",
    "            \"data_source\": \"https://github.com/suwangcompling/Modeling-Semantic-Plausibility-NAACL18/tree/master/data\",\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"commonse-reasoning\",\n",
    "            \"answer-type\": \"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain score 1 (for True) or 0 (for False), based on gold labels from original data. The average score serves as overall accuracy. \\\",\n",
    "            \"note\": \"download two files pos-all.txt and neg-all.txt to ./data/pep-3k/\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    pos_data = open('data/pep-3k/pos-all.txt', 'r').read().splitlines()\n",
    "    neg_data = open('data/pep-3k/neg-all.txt', 'r').read().splitlines()\n",
    "    num_data_each = num_dataset // 2\n",
    "    test_examples = pos_data[:num_data_each] + neg_data[:num_data_each]\n",
    "    \n",
    "    test_ids = list(range(num_data_each))*2\n",
    "    test_golds = ['true']*num_data_each + ['false']*num_data_each\n",
    "\n",
    "    if output_gold:\n",
    "        return test_examples, [['true', 'false'] for i in range(len(test_ids))], test_ids, test_golds\n",
    "    else:\n",
    "        return test_examples, [['true', 'false'] for i in range(len(test_ids))], test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "cc246a0b-06ac-47fd-a04f-9cfa1332b264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries, choices, ids, labels = pep_3k(num_dataset=3080)\n",
    "queries = list(map(lambda x: f'Is it true that: {x}?', queries))\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "71e098ea-52f0-434f-ba3c-2894e4b90e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def step_game(exp_type, num_dataset=30): # ['hard', 'basic', 'clock-position', 'basic-cardinal', 'diagonal']\n",
    "    \"\"\"\n",
    "        StepGame - Spatial Reasoing & Question Answeing\n",
    "\n",
    "        {\n",
    "            \"data_source\": [\"https://github.com/ZhengxiangShi/StepGame/blob/main/Dataset/CompleteVersion/clean/qa1_test.json\",\n",
    "                \"https://github.com/ZhengxiangShi/StepGame/blob/main/Dataset/CompleteVersion/clean/qa9_valid.json\"],\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"spatial-reasoning\",\n",
    "            \"answer-type\":\"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    if exp_type == 'hard':\n",
    "        path = 'data/qa9_valid.json'\n",
    "    elif exp_type in ['basic', 'clock-position', 'basic-cardinal', 'diagonal']:\n",
    "        path = 'data/qa1_test.json'\n",
    "    \n",
    "    f = open(path)\n",
    "    stepgame_data = json.load(f)\n",
    "\n",
    "    MC_text = \"Choose from: left, right, above, below, lower-left, lower-right, upper-left, upper-right.\"\n",
    "    \n",
    "    if exp_type == 'basic':\n",
    "        test_ids = [i for i in range(num_dataset+1)]\n",
    "        test_ids.remove(4)  # gold label is wrong\n",
    "    elif exp_type == 'diagonal': \n",
    "        test_ids = [30, 31, 39, 40, 41, 47, 48, 50, 52, 53, 60, 61, 63, 65, 67, 69, 71, 72, 84, 87]\n",
    "    elif exp_type == 'clock-position':\n",
    "        test_ids = [62, 54, 119, 131, 143, 144, 189, 316, 426, 484, 697, 820, 960, 1045, 1163, 1607, 1618, 1620, 1736, 1778]\n",
    "    elif exp_type == 'basic-cardinal':\n",
    "        test_ids = [1, 2, 3, 9, 10, 16, 17, 18, 19, 22, 25, 27, 33, 34, 35, 42, 49, 51, 59, 66]\n",
    "    elif exp_type == 'hard':\n",
    "        test_ids = [i for i in range(num_dataset)]\n",
    "    \n",
    "    test_examples, test_golds = [], []\n",
    "    for i in test_ids:\n",
    "        # ex = [stepgame_data[str(i)]]\n",
    "        ex = stepgame_data[str(i)]\n",
    "        if exp_type == 'hard':\n",
    "            ex['input'] = \"Given the description: {}. {}\".format(\n",
    "                ' '.join(ex['story']), ex['question'].replace('relation', 'spatial relation, (e.g left, right, above lower-left, ..)'))\n",
    "       \n",
    "        else:\n",
    "            ex['input'] = f\"{ex['story'][0]} {ex['question']} {MC_text}\"\n",
    "        test_examples.append(ex['input'])\n",
    "        test_golds.append(ex['label'])\n",
    "    return test_examples, test_ids, test_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "7d6df130-396d-4f15-a8e9-0840e2653ca0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "queries, ids, labels = step_game(exp_type='basic', num_dataset=1000)\n",
    "choices = [list(set(labels))] * 1000\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "8e5d1f33-b789-4b7d-a5ef-ef438adabc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "queries, ids, labels = step_game(exp_type='hard', num_dataset=1000)\n",
    "choices = [list(set(labels))] * 1000\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f76bd084-6b02-4cf9-aee9-b4d33858979b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def letter_string_analogies(): \n",
    "#     \"\"\"\n",
    "#         Letter_string_analogies - Analogical Reasoing\n",
    "#         {\n",
    "#             \"data_source\": [\"https://github.com/taylorwwebb/emergent_analogies_LLM/blob/main/letter_string/all_prob.npz\"],\n",
    "#             \"evaluation-method\": \"human-evaluation\",\n",
    "#             \"evaluation-aspect\": \"analogical-reasoning\",\n",
    "#             \"answer-type\":\"\",\n",
    "#             \"evaluation-details\": \"Feed the prompt and obtain the answer from ChatGPT and compare with gold.\"\n",
    "#         }\n",
    "#     \"\"\"\n",
    "#     # Load all problems\n",
    "#     file ='data/all_prob.npz'\n",
    "#     all_prob = np.load(file, allow_pickle=True)['all_prob']\n",
    "#     all_completions = np.load(file, allow_pickle=True)['all_prob_completion']\n",
    "\n",
    "#     test_examples,test_ids,test_golds = [],[],[]\n",
    "\n",
    "#     # problem types 4 and 5\n",
    "#     for p in [4, 5]:\n",
    "#         probs = all_prob[p][:15]\n",
    "#         golds = all_completions[p][:15]\n",
    "#         for prob, ans in zip(probs, golds):\n",
    "#             prompt = \"Let's try to complete the pattern:\\n\\n\" + prob\n",
    "#             test_examples.append(prompt)\n",
    "#             test_golds.append(ans)\n",
    "    \n",
    "#     return test_examples, test_ids, test_golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "252b2f14-e6f8-42af-8a17-b874c2162264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# queries, ids, labels = letter_string_analogies()\n",
    "\n",
    "# dset = Dataset.from_dict({\n",
    "#     'id': ids,\n",
    "#     'question': queries,\n",
    "#     'label': labels\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "af296a4a-9bfd-4efa-8c79-42a62de62ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def babi(exp_type = 15, prompt_engineering=False, num_dataset=30, batching=True, output_gold=True, save_csv=False):\n",
    "    \"\"\"\n",
    "        bAbI\n",
    "        This basic induction bAbI tasks is taken from the (20) QA bAbI tasks that a set of proxy tasks that evaluate \n",
    "        reading comprehension via question answering. The tasks measure understanding in several ways: whether a system \n",
    "        is able to answer questions via simple induction. \n",
    "        The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human.\n",
    "    \n",
    "        {\n",
    "            \"data_source\": [\"http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\"],\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"deductive-inductive reasoning\",\n",
    "            \"answer-type\":\"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    def make_samples(filename, exp_type, prompt_engineering=False, batching=True):\n",
    "        df = pd.read_csv(filename, header=None)\n",
    "    \n",
    "        dataset = {}\n",
    "        index = 0\n",
    "        golds = {}\n",
    "        for i, row in df.itertuples():\n",
    "            if index not in dataset.keys():\n",
    "                dataset[index] = ''\n",
    "                golds[index] = ''\n",
    "\n",
    "            if 'what' in row.lower():\n",
    "                \n",
    "                gold = row[row.find('\\t'):]\n",
    "                row = row[:row.find('\\t')]\n",
    "                \n",
    "                if batching:\n",
    "                    dataset[index] += row + '\\n'\n",
    "                    golds[index] += gold + '\\n'\n",
    "                    \n",
    "                    if i+1 < df.shape[0]:\n",
    "                        if prompt_engineering and \\\n",
    "                            'what' not in df.iloc[[i+1]].values[0][0].lower():\n",
    "                            if exp_type == 15:\n",
    "                                marker = 'deductive' \n",
    "                            elif exp_type == 16:\n",
    "                                marker = 'inductive' \n",
    "                              \n",
    "                            allprev = ''.join([i for i in dataset[index].replace('\\n', '') if not i.isdigit()]).strip()\n",
    "                            context = allprev[:allprev.lower().find('what')]\n",
    "                            question = allprev[allprev.lower().find('what'):]\n",
    "\n",
    "                            dataset[index] = 'Given facts: ' + context +\\\n",
    "                                             '\\n\\nThe most recent fact is the correct fact.\\n\\nBased on the given facts above, do a reasonable inference on this question using '+marker+' reasoning: ' + question\n",
    "                        \n",
    "                    if i+1 < df.shape[0]:\n",
    "                        if 'what' not in df.iloc[[i+1]].values[0][0].lower():\n",
    "                            index += 1\n",
    "                    else:\n",
    "                        index += 1\n",
    "                else:\n",
    "                    golds[index] = gold\n",
    "                    if 'what' not in df.iloc[[i-1]].values[0][0].lower():\n",
    "                        context = dataset[index]\n",
    "\n",
    "                    if not prompt_engineering:\n",
    "                        dataset[index] = context + row\n",
    "                    else:\n",
    "                        if exp_type == 15:\n",
    "                            marker = 'deductive' \n",
    "                        elif exp_type == 16:\n",
    "                            marker = 'inductive' \n",
    "\n",
    "                        dataset[index] = 'Given facts: ' +\\\n",
    "                                         ''.join([i for i in context.replace('\\n', '') if not i.isdigit()]).strip() +\\\n",
    "                                         '\\n\\nThe most recent fact is the correct fact.\\n\\nBased on the given facts above, do a reasonable inference on this question using '+marker+' reasoning: ' +\\\n",
    "                                         alphaonly.sub('', row).strip()\n",
    "\n",
    "                    index += 1\n",
    "                    \n",
    "            else:\n",
    "                dataset[index] += row + '\\n'\n",
    "                \n",
    "        ids = dataset.keys()\n",
    "        dataset_in_list = [dataset[idx] for idx in ids]\n",
    "        gold_in_list = [golds[idx] for idx in ids]\n",
    "        \n",
    "        return dataset_in_list, gold_in_list\n",
    "    \n",
    "    if exp_type == 15:\n",
    "        filename = 'tasks_1-20_v1-2/en/qa15_basic-deduction_test.txt'\n",
    "    elif exp_type == 16:\n",
    "        filename = 'tasks_1-20_v1-2/en/qa16_basic-induction_test.txt'\n",
    "    else:\n",
    "        raise NotImplementedError('task_id: {} is not yet implemented'.\\\n",
    "                                            format(exp_type))\n",
    "        \n",
    "    filename = 'data/' + filename\n",
    "    if not os.path.isfile(filename):\n",
    "        os.system('wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz')\n",
    "        os.system('tar -xf tasks_1-20_v1-2.tar.gz')\n",
    "        os.system('rm -rf tasks_1-20_v1-2.tar.gz')\n",
    "        os.system('mv tasks_1-20_v1-2 data/')\n",
    "    \n",
    "    prompts, golds = make_samples(filename, exp_type, prompt_engineering, batching)\n",
    "    \n",
    "    df = pd.DataFrame({'Ids':list(range(num_dataset)), 'Prompts':prompts[:num_dataset], 'Golds': list(map(lambda x: x.split('\\t')[1], golds[:num_dataset])) })\n",
    "    test_examples = df.Prompts.tolist()\n",
    "    test_ids = df.Ids.tolist()\n",
    "    test_golds = df.Golds.tolist()\n",
    "    if save_csv:\n",
    "        df.to_csv('babi_task_'+str(exp_type)+'.csv', index=False)\n",
    "    \n",
    "    if output_gold:\n",
    "        return test_examples, test_ids, test_golds\n",
    "    else:\n",
    "        return test_examples, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "8a234514-1b8e-43ae-979a-631791b97ce3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries, ids, labels = babi(exp_type=15, prompt_engineering=True, num_dataset=1000, batching=False, output_gold=True)\n",
    "choices = [list(set(labels))] * 1000\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "eacf60a3-19d6-4f66-85fb-691725ec85f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries, ids, labels = babi(exp_type=16, prompt_engineering=True, num_dataset=1000, batching=False, output_gold=True)\n",
    "choices = [list(set(labels))] * 1000\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "10f60d17-8ed1-4a11-902a-f6518cba4663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def entailmentbank(num_dataset=30, output_gold=True, save_csv=False):\n",
    "#     \"\"\"\n",
    "#         EntailmentBank\n",
    "#         ENTAILMENTBANK, the first dataset of multistep entailment trees for QA, to support entailment-based explanation. \n",
    "#         ENTAILMENTBANK contains two parts: 1,840 entailment trees, each tree showing how a question-answer pair (QA) is \n",
    "#         entailed from a small number of relevant sentences (e.g., Figure 1); and a general corpus C, containing those and \n",
    "#         other sentences of domain-specific and general knowledge relevant to the QA domain.\n",
    "    \n",
    "#         {\n",
    "#             \"data_source\": [\"https://drive.google.com/drive/folders/1SmnCw-Dfad3a68AmZZtG4jBz6UUKFkZk\", \n",
    "#                             \"https://allenai.org/data/entailmentbank\",\n",
    "#                             \"https://github.com/allenai/entailment_bank/\"],\n",
    "#             \"evaluation-method\": \"human-evaluation\",\n",
    "#             \"evaluation-aspect\": \"deductive reasoning\",\n",
    "#             \"answer-type\":\"\",\n",
    "#             \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "#                                 The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\"        }\n",
    "#     \"\"\"\n",
    "#     if not os.path.isfile(\"data/entailment_trees_emnlp2021_data_v3/dataset/task_1/test.jsonl\"):\n",
    "#         url = \"https://drive.google.com/drive/folders/1SmnCw-Dfad3a68AmZZtG4jBz6UUKFkZk\"\n",
    "#         gdown.download_folder(url, quiet=True, use_cookies=False)\n",
    "\n",
    "#         os.system('unzip v3_May6_2022/entailment_trees_emnlp2021_data_v3.zip')\n",
    "#         os.system('rm -rf v3_May6_2022/')\n",
    "#         os.system('mv entailment_trees_emnlp2021_data_v3 data/')\n",
    "    \n",
    "#     entailmentbank = load_dataset(\"json\", data_files=\"data/entailment_trees_emnlp2021_data_v3/dataset/task_1/test.jsonl\")\n",
    "    \n",
    "#     prompts = []\n",
    "#     golds = []\n",
    "#     for dataset_id in range(num_dataset):\n",
    "#         data = entailmentbank['train'][dataset_id]\n",
    "#         context = \". \".join([sent[3:].strip() for sent in data['context'].split('sent')[1:]])\n",
    "#         question = data['question']\n",
    "#         gold = data['answer']\n",
    "\n",
    "#         prompt = context + '. ' + question\n",
    "\n",
    "#         prompts.append(prompt)\n",
    "#         golds.append(gold)\n",
    "    \n",
    "#     df = pd.DataFrame({'Ids':list(range(num_dataset)), 'Prompts':prompts, 'Golds':golds})\n",
    "#     test_examples = df.Prompts.tolist()\n",
    "#     test_ids = df.Ids.tolist()\n",
    "#     test_golds = df.Golds.tolist()\n",
    "#     if save_csv:\n",
    "#         df.to_csv('entailmentbank.csv', index=False)\n",
    "    \n",
    "#     if output_gold:\n",
    "#         return test_examples, test_ids, test_golds\n",
    "#     else:\n",
    "#         return test_examples, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "d974918c-463e-495f-ab20-1e34a44fc478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# queries, ids, labels = entailmentbank()\n",
    "\n",
    "# dset = Dataset.from_dict({\n",
    "#     'id': ids,\n",
    "#     'question': queries,\n",
    "#     # 'choice': choices,\n",
    "#     'label': labels\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "e43c7bd9-0f8f-461c-91c4-ee381663e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_nli(num_dataset=30, output_gold=True, save_csv=False):\n",
    "    \"\"\"\n",
    "        αNLI\n",
    "        αbductive Natural Language Inference (αNLI) is a new commonsense benchmark dataset designed to test \n",
    "        an AI system’s capability to apply abductive reasoning and common sense to form possible explanations for \n",
    "        a given set of observations. Formulated as a binary-classification task, the goal is to pick the most \n",
    "        plausible explanatory hypothesis given two observations from narrative contexts.\n",
    "    \n",
    "        {\n",
    "            \"data_source\": [\"https://storage.googleapis.com/ai2-mosaic/public/abductive-commonsense-reasoning-iclr2020/anli.zip\",\n",
    "                            \"http://abductivecommonsense.xyz/\"],\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"abductive reasoning\",\n",
    "            \"answer-type\":\"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isfile('anli/test.jsonl'):\n",
    "        os.system('wget https://storage.googleapis.com/ai2-mosaic/public/abductive-commonsense-reasoning-iclr2020/anli.zip')\n",
    "        os.system('/usr/bin/unzip anli.zip')\n",
    "        os.system('rm -rf anli.zip')\n",
    "        os.system('mv anli data/')\n",
    "    \n",
    "    anli_dataset = load_dataset('json', data_files='data/anli/test.jsonl')\n",
    "    lines = loadtxt('data/anli/test-labels.lst', comments=\"#\", delimiter=\",\", unpack=False)\n",
    "    labels = [int(line) for line in lines]\n",
    "    \n",
    "    prompts = []\n",
    "    golds = []\n",
    "    for dataset_id in range(num_dataset):\n",
    "        data = anli_dataset['train'][dataset_id]\n",
    "\n",
    "        prompt = 'Given: ' + data['obs1'] + ' Then: '+ data['obs2'] + \\\n",
    "                ' Select the most plausible explanation (hypothesis): A. ' + data['hyp1'] + \\\n",
    "                ' B. ' + data['hyp2']\n",
    "\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    df = pd.DataFrame({'Ids':list(range(num_dataset)), 'Prompts':prompts, 'Golds':labels[:num_dataset]})\n",
    "    test_examples = df.Prompts.tolist()\n",
    "    test_ids = df.Ids.tolist()\n",
    "    test_golds = df.Golds.tolist()\n",
    "    if save_csv:\n",
    "        df.to_csv('anli.csv', index=False)\n",
    "    \n",
    "    if output_gold:\n",
    "        return test_examples, test_ids, test_golds\n",
    "    else:\n",
    "        return test_examples, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "09076fdc-8135-4961-b8dd-6b3aa599fd37",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries, ids, labels = alpha_nli(num_dataset = 3059)\n",
    "choices = [['A', 'B'] for i in range(3059)]\n",
    "labels = list(map(lambda x: 'A' if x == 1 else 'B', labels))\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "93118ce3-1d20-409e-82e9-9b0e659fca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clutrr(num_dataset=30, output_gold=True, save_csv=False):\n",
    "    \"\"\"\n",
    "        CLUTRR\n",
    "        CLUTRR (Compositional Language Understanding and Text-based Relational Reasoning), \n",
    "        a diagnostic benchmark suite, is first introduced in (https://arxiv.org/abs/1908.06177) \n",
    "        to test the systematic generalization and inductive reasoning capabilities of NLU systems. \n",
    "        The CLUTRR benchmark allows us to test a model’s ability for systematic generalization by \n",
    "        testing on stories that contain unseen combinations of logical rules, and test for the \n",
    "        various forms of model robustness by adding different kinds of superfluous noise facts to the stories.\n",
    "    \n",
    "        {\n",
    "            \"data_source\": [\"https://huggingface.co/datasets/CLUTRR/v1\"],\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"inductive reasoning\",\n",
    "            \"answer-type\":\"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    clutrr_dataset = load_dataset(\"CLUTRR/v1\", \"gen_train23_test2to10\")\n",
    "    \n",
    "    prompts = []\n",
    "    golds = []\n",
    "    for dataset_id in range(num_dataset):\n",
    "        data = clutrr_dataset['test'][dataset_id]\n",
    "        first_p = eval(data['query'])[1]\n",
    "        second_p = eval(data['query'])[0]\n",
    "\n",
    "        prompt = data['clean_story'] + '. Who is ' + first_p + ' to ' + second_p + '?'\n",
    "        gold = data['target_text']\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        golds.append(gold)\n",
    "\n",
    "    df = pd.DataFrame({'Ids':list(range(num_dataset)), 'Prompts':prompts, 'Golds':golds})\n",
    "    test_examples = df.Prompts.tolist()\n",
    "    test_ids = df.Ids.tolist()\n",
    "    test_golds = df.Golds.tolist()\n",
    "    if save_csv:\n",
    "        df.to_csv('clutrr.csv', index=False)\n",
    "    \n",
    "    if output_gold:\n",
    "        return test_examples, test_ids, test_golds\n",
    "    else:\n",
    "        return test_examples, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "4c1cd81e-7ff4-4230-a960-8c139ee0324b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries, ids, labels = clutrr(num_dataset=1146)\n",
    "choices = [list(set(labels))] * 1146\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "54e3bc82-2c46-4892-8549-4f0455084ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def commonsenseqa(num_dataset=30, output_gold=True, save_csv=False):\n",
    "    \"\"\"\n",
    "        CommonsenseQA\n",
    "        CommonsenseQA is a new multiple-choice question answering dataset that requires different types of \n",
    "        commonsense knowledge to predict the correct answers . It contains 12,102 questions with one correct \n",
    "        answer and four distractor answers. The dataset is provided in two major training/validation/testing \n",
    "        set splits: \"Random split\" which is the main evaluation split, and \"Question token split\", see paper for details.\n",
    "    \n",
    "        {\n",
    "            \"data_source\": [\"https://huggingface.co/datasets/commonsense_qa\"],\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"commonsense reasoning\",\n",
    "            \"answer-type\":\"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    commonsenseqa_dataset = load_dataset(\"commonsense_qa\")\n",
    "    \n",
    "    prompts = []\n",
    "    golds = []\n",
    "    for i in range(num_dataset):\n",
    "\n",
    "        gold = commonsenseqa_dataset['validation'][i]['answerKey']\n",
    "        prompt = commonsenseqa_dataset['validation'][i]['question']\n",
    "\n",
    "        choice_str = ''\n",
    "        for choice_id, choice in enumerate(commonsenseqa_dataset['validation'][i]['choices']['label']):\n",
    "            choice_str += choice + '. ' + commonsenseqa_dataset['validation'][i]['choices']['text'][choice_id] + ', '\n",
    "        prompt += ' ' + choice_str[:-2]\n",
    "        prompt\n",
    "\n",
    "        prompts.append(prompt)\n",
    "        golds.append(gold)\n",
    "    \n",
    "    df = pd.DataFrame({'Ids':list(range(num_dataset)), 'Prompts':prompts, 'Golds':golds})\n",
    "    test_examples = df.Prompts.tolist()\n",
    "    test_ids = df.Ids.tolist()\n",
    "    test_golds = df.Golds.tolist()\n",
    "    if save_csv:\n",
    "        df.to_csv('commonsense_qa.csv', index=False)\n",
    "    \n",
    "    if output_gold:\n",
    "        return test_examples, test_ids, test_golds\n",
    "    else:\n",
    "        return test_examples, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "17ed1a47-172b-4c5b-921e-286ce5f3c047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries, ids, labels = commonsenseqa(num_dataset=1221)\n",
    "choices = [list(set(labels))] * 1221\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "95a14ea4-1f51-42df-9cfb-aabc217b9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def piqa(num_dataset=30, output_gold=True, save_csv=False):\n",
    "#     \"\"\"\n",
    "#         PIQA\n",
    "#         To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this \n",
    "#         kind of physical commonsense pose a challenge to state-of-the-art natural language understanding systems. \n",
    "#         The PIQA dataset introduces the task of physical commonsense reasoning and a corresponding benchmark \n",
    "#         dataset Physical Interaction: Question Answering or PIQA. Physical commonsense knowledge is a major challenge \n",
    "#         on the road to true AI-completeness, including robots that interact with the world and understand natural language. \n",
    "#         PIQA focuses on everyday situations with a preference for atypical solutions. The dataset is inspired by \n",
    "#         instructables.com, which provides users with instructions on how to build, craft, bake, or manipulate objects\n",
    "#         using everyday materials.\n",
    "    \n",
    "#         {\n",
    "#             \"data_source\": [\"https://huggingface.co/datasets/piqa\"],\n",
    "#             \"evaluation-method\": \"human-evaluation\",\n",
    "#             \"evaluation-aspect\": \"commonsense reasoning\",\n",
    "#             \"answer-type\":\"\",\n",
    "#             \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "#                                 The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\"\n",
    "#         }\n",
    "#     \"\"\"\n",
    "#     piqa_dataset = load_dataset(\"piqa\")\n",
    "    \n",
    "#     prompts = []\n",
    "#     golds = []\n",
    "#     for i in range(num_dataset):\n",
    "\n",
    "#         gold = piqa_dataset['validation'][i]['label']\n",
    "\n",
    "#         if piqa_dataset['validation'][i]['goal'][-1]=='.':\n",
    "#             goal = piqa_dataset['validation'][i]['goal'][:-1]\n",
    "#         else:\n",
    "#             goal = piqa_dataset['validation'][i]['goal']\n",
    "\n",
    "#         prompt = 'Pick from option 0 or 1 to achieve this goal:' + goal + ' 0: \"' +\\\n",
    "#                  piqa_dataset['validation'][i]['sol1'] + '\" 1: \"' +\\\n",
    "#                  piqa_dataset['validation'][i]['sol2'] + '\"'\n",
    "\n",
    "#         prompts.append(prompt)\n",
    "#         golds.append(gold)\n",
    "\n",
    "#     df = pd.DataFrame({'Ids':list(range(num_dataset)), 'Prompts':prompts, 'Golds': list(map(lambda x: str(x), golds)) })\n",
    "#     test_examples = df.Prompts.tolist()\n",
    "#     test_ids = df.Ids.tolist()\n",
    "#     test_golds = df.Golds.tolist()\n",
    "#     if save_csv:\n",
    "#         df.to_csv('piqa.csv', index=False)\n",
    "    \n",
    "#     if output_gold:\n",
    "#         return test_examples, test_ids, test_golds\n",
    "#     else:\n",
    "#         return test_examples, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "28925592-527a-4e17-8e79-f6679a9c9c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# queries, ids, labels = piqa(num_dataset=1838)\n",
    "# choices = [['0', '1']] * 1838\n",
    "\n",
    "# dset = Dataset.from_dict({\n",
    "#     'id': ids,\n",
    "#     'question': queries,\n",
    "#     'choice': choices,\n",
    "#     'label': labels\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "394c5faa-f798-4ac2-911e-7655eaa8fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecare(num_dataset=30, output_gold=True, save_csv=False):\n",
    "    \"\"\"\n",
    "        E-Care\n",
    "        Understanding causality has vital importance for various Natural Language Processing (NLP) applications. \n",
    "        Beyond the labeled instances, conceptual explanations of the causality can provide a deep understanding \n",
    "        of the causal fact to facilitate the causal reasoning process. We present a human-annotated explainable \n",
    "        CAusal REasoning dataset (e-CARE), which contains over 20K causal reasoning questions, together with \n",
    "        natural language formed explanations of the causal questions.\n",
    "    \n",
    "        {\n",
    "            \"data_source\": [\"https://huggingface.co/datasets/12ml/e-CARE\"],\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"evaluation-aspect\": \"causal reasoning\",\n",
    "            \"answer-type\":\"\",\n",
    "            \"evaluation-details\": \"Feed the 'input' of the examples to the model and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\"\n",
    "        }\n",
    "    \"\"\"\n",
    "    ecare_dataset = load_dataset(\"12ml/e-CARE\")\n",
    "    \n",
    "    prompts = []\n",
    "    golds = []\n",
    "    gold_explanations = []\n",
    "\n",
    "    for i in range(num_dataset):\n",
    "\n",
    "        gold = ecare_dataset['validation'][i]['label']\n",
    "        gold_explanation = ecare_dataset['validation'][i]['conceptual_explanation']\n",
    "\n",
    "        if ecare_dataset['validation'][i]['question'] == 'cause':\n",
    "            prompt = 'Choices:\\nA: ' + \\\n",
    "                        ecare_dataset['validation'][i]['choice1'] + ' B: ' + \\\n",
    "                        ecare_dataset['validation'][i]['choice2'] + '\\nWhich one of the choices are causing the sentence: ' + \\\n",
    "                        ecare_dataset['validation'][i]['premise']\n",
    "        elif ecare_dataset['validation'][i]['question'] == 'effect':\n",
    "            prompt = 'If ' + ecare_dataset['validation'][i]['premise'] + ' Which one of the choices are caused by that? Choices:\\nA: ' + \\\n",
    "                        ecare_dataset['validation'][i]['choice1'] + ' B: ' + \\\n",
    "                        ecare_dataset['validation'][i]['choice2']\n",
    "        prompts.append(prompt)\n",
    "        golds.append(gold)\n",
    "        gold_explanations.append(gold_explanation)\n",
    "\n",
    "    df = pd.DataFrame({'Ids':list(range(num_dataset)), 'Prompts':prompts, 'Golds':list(map(lambda x: str(x), golds)), 'Gold_explanations':gold_explanations})\n",
    "    \n",
    "    test_examples = df.Prompts.tolist()\n",
    "    test_ids = df.Ids.tolist()\n",
    "    test_golds = df.Golds.tolist()\n",
    "    if save_csv:\n",
    "        df.to_csv('ecare.csv', index=False)\n",
    "    \n",
    "    if output_gold:\n",
    "        return test_examples, test_ids, test_golds\n",
    "    else:\n",
    "        return test_examples, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "ece3cd79-f12d-4596-ac37-6afc9f89e721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries, ids, labels = ecare(num_dataset=2122)\n",
    "choices = [['A', 'B']] * 2122\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "46ea0054-cdff-43af-af5c-b1a3d9be66f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def covid_factchecking(exp_type='scientific'): # ['scientific', 'social']\n",
    "    \"\"\"\n",
    "        Covid-factchecking - Hallucination & Factuality \n",
    "            - function for both Covid-social and Covid-scientific\n",
    "        {\n",
    "            \"data_source\": [\"https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/fact_checker/covid19_scientific/task.json\",\n",
    "                            \"https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/fact_checker/politifact/task.json\"],\n",
    "            \"evaluation-method\": \"human-evaluation\",\n",
    "            \"answer-type\": \"true/false\",\n",
    "            \"evaluation-details\": \"Feed the example to the model without any instruction and take generated answer for evaluation.\\\n",
    "                                The generated answer is evaluated by a human to obtain accuracy, based on gold labels from original data.\\\n",
    "        }\n",
    "    \"\"\"\n",
    "    if exp_type == 'scientific':\n",
    "        path = 'data/covid19_scientific.json'\n",
    "    elif exp_type =='social':\n",
    "        path = 'data/politifact.json'\n",
    "    \n",
    "    f = open(path)\n",
    "    covid_data = json.load(f)\n",
    "\n",
    "    count_true, count_false = 0, 0\n",
    "    test_examples, test_ids = [], []\n",
    "\n",
    "    for ex in covid_data['examples']:\n",
    "        label = 'true' if ex['target_scores']['true'] == 1 else 'false'\n",
    "        if label == 'true':\n",
    "            count_true+=1\n",
    "        else:\n",
    "            count_false+=1\n",
    "        test_examples.append(ex)\n",
    "        test_ids.append(ex['id'])\n",
    "    \n",
    "    return test_examples, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "77edafb5-a0a1-4b01-b8bc-f44bbd276d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples, ids = covid_factchecking('scientific')\n",
    "queries = [ex['input'] for ex in examples]\n",
    "labels = ['true' if ex['target_scores']['true'] else 'false' for ex in examples]\n",
    "choices = [['true', 'false'] for ex in examples]\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "9a7ec850-049b-4d83-bfe8-f3e31e224393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "examples, ids = covid_factchecking('social')\n",
    "queries = [ex['input'] for ex in examples]\n",
    "labels = ['true' if ex['target_scores']['true'] else 'false' for ex in examples]\n",
    "choices = [['true', 'false'] for ex in examples]\n",
    "\n",
    "dset = Dataset.from_dict({\n",
    "    'id': ids,\n",
    "    'question': queries,\n",
    "    'choice': choices,\n",
    "    'label': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7554055-175f-4a0e-bce4-7485ec135c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_indot0)",
   "language": "python",
   "name": "env_indot0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
