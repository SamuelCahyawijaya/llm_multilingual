{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667a0cb2-7018-4c03-a15a-622035124005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "072dac52-991f-4562-82d6-0de0565db138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistics(pred1, pred2, n=1000):\n",
    "    acc1, acc2 = sum(pred1) / len(pred1), sum(pred2) / len(pred2)\n",
    "    if acc1 > acc2:\n",
    "        anchor = 1\n",
    "    else:\n",
    "        anchor = 2\n",
    "    ref_diff_acc = acc1 - acc2 if anchor == 1 else acc2 - acc1\n",
    "    preds = pred1 + pred2\n",
    "    \n",
    "    occurences = 0\n",
    "    for i in range(n):\n",
    "        pred_x, pred_y = train_test_split(preds, test_size=len(pred2), random_state=i)\n",
    "        acc_x, acc_y = sum(pred_x) / len(pred_x), sum(pred_y) / len(pred_y)\n",
    "        diff_acc = acc_x - acc_y if anchor == 1 else acc_y - acc_x\n",
    "        if diff_acc >= ref_diff_acc:\n",
    "            occurences += 1\n",
    "    \n",
    "    return occurences / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34efbe-b006-491f-b374-2fb9926f5296",
   "metadata": {},
   "source": [
    "# ChatGPT Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f22c05f9-9453-4572-b614-cab423047638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_correct(row):\n",
    "    if str(row['Gold'])[0] == '[':\n",
    "        return str(row['Pred']) in map(lambda x: str(x), eval(row['Gold']))\n",
    "    else:\n",
    "        return str(row['Pred']) == str(row['Gold'])\n",
    "\n",
    "pred_data = {}\n",
    "acc_data = {'task': [], 'model': [], 'accuracy': []}\n",
    "for path in glob('outputs_chatgpt_eval/*.csv'):\n",
    "    task = path.split('/')[-1].split('_falcon-')[0]\n",
    "    model_name = 'falcon-' + path.split('/')[-1].split('_falcon-')[1][:-4]    \n",
    "    if ('lang' not in model_name) or ('covid' in task):\n",
    "        continue\n",
    "    model, n_lang = model_name.split('-lang-')\n",
    "        \n",
    "    df = pd.read_csv(path)\n",
    "    if task == 'ecare':\n",
    "        df['Gold'] = df['Gold'].apply(lambda x: 'A' if str(x) == '0' else 'B')\n",
    "    df['correct'] = df.apply(check_correct, axis='columns')\n",
    "    \n",
    "    if task not in pred_data:\n",
    "        pred_data[task] = {}    \n",
    "    if model not in pred_data[task]:\n",
    "        pred_data[task][model] = {}\n",
    "    pred_data[task][model][int(n_lang)] = df['correct'].tolist()\n",
    "    \n",
    "    acc_data['task'].append(task)\n",
    "    acc_data['model'].append(model_name)\n",
    "    acc_data['accuracy'].append(df['correct'].sum() / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9aa6164-f829-4583-816f-0971e57f536f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>task</th>\n",
       "      <th>alpha_nli</th>\n",
       "      <th>babi15</th>\n",
       "      <th>babi16</th>\n",
       "      <th>clutrr</th>\n",
       "      <th>commonsenseqa</th>\n",
       "      <th>ecare</th>\n",
       "      <th>pep_3k</th>\n",
       "      <th>sparta_qa_1reasoning</th>\n",
       "      <th>sparta_qa_2reasoning</th>\n",
       "      <th>step_game_basic</th>\n",
       "      <th>step_game_hard</th>\n",
       "      <th>timedial</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>falcon-40b-lang-1</th>\n",
       "      <td>0.783589</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.296684</td>\n",
       "      <td>0.719902</td>\n",
       "      <td>0.662111</td>\n",
       "      <td>0.561039</td>\n",
       "      <td>0.417625</td>\n",
       "      <td>0.327189</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.269710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-40b-lang-10</th>\n",
       "      <td>0.735861</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.247818</td>\n",
       "      <td>0.525799</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.535065</td>\n",
       "      <td>0.402299</td>\n",
       "      <td>0.373272</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.494467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-40b-lang-45</th>\n",
       "      <td>0.631906</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.243455</td>\n",
       "      <td>0.447993</td>\n",
       "      <td>0.614515</td>\n",
       "      <td>0.513312</td>\n",
       "      <td>0.421456</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.251037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-1</th>\n",
       "      <td>0.618830</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.199825</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>0.562677</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.325670</td>\n",
       "      <td>0.336406</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.331950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-10</th>\n",
       "      <td>0.525989</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.196335</td>\n",
       "      <td>0.209664</td>\n",
       "      <td>0.515080</td>\n",
       "      <td>0.521753</td>\n",
       "      <td>0.352490</td>\n",
       "      <td>0.331797</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.822960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-2</th>\n",
       "      <td>0.575678</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.198953</td>\n",
       "      <td>0.237510</td>\n",
       "      <td>0.511781</td>\n",
       "      <td>0.524026</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.368664</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.483402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-20</th>\n",
       "      <td>0.517489</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.158813</td>\n",
       "      <td>0.187551</td>\n",
       "      <td>0.510368</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.302682</td>\n",
       "      <td>0.359447</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.159751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-3</th>\n",
       "      <td>0.516509</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.185864</td>\n",
       "      <td>0.213759</td>\n",
       "      <td>0.521678</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.331797</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.878976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-45</th>\n",
       "      <td>0.520105</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.121291</td>\n",
       "      <td>0.192465</td>\n",
       "      <td>0.518379</td>\n",
       "      <td>0.497078</td>\n",
       "      <td>0.283525</td>\n",
       "      <td>0.327189</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.271093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-5</th>\n",
       "      <td>0.519124</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.169284</td>\n",
       "      <td>0.206388</td>\n",
       "      <td>0.509425</td>\n",
       "      <td>0.531494</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.345622</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.470954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task                alpha_nli  babi15  babi16    clutrr  commonsenseqa  \\\n",
       "model                                                                    \n",
       "falcon-40b-lang-1    0.783589   0.224   0.621  0.296684       0.719902   \n",
       "falcon-40b-lang-10   0.735861   0.227   0.504  0.247818       0.525799   \n",
       "falcon-40b-lang-45   0.631906   0.259   0.549  0.243455       0.447993   \n",
       "falcon-7b-lang-1     0.618830   0.207   0.518  0.199825       0.260442   \n",
       "falcon-7b-lang-10    0.525989   0.216   0.508  0.196335       0.209664   \n",
       "falcon-7b-lang-2     0.575678   0.302   0.518  0.198953       0.237510   \n",
       "falcon-7b-lang-20    0.517489   0.227   0.491  0.158813       0.187551   \n",
       "falcon-7b-lang-3     0.516509   0.240   0.511  0.185864       0.213759   \n",
       "falcon-7b-lang-45    0.520105   0.231   0.517  0.121291       0.192465   \n",
       "falcon-7b-lang-5     0.519124   0.211   0.496  0.169284       0.206388   \n",
       "\n",
       "task                   ecare    pep_3k  sparta_qa_1reasoning  \\\n",
       "model                                                          \n",
       "falcon-40b-lang-1   0.662111  0.561039              0.417625   \n",
       "falcon-40b-lang-10  0.639491  0.535065              0.402299   \n",
       "falcon-40b-lang-45  0.614515  0.513312              0.421456   \n",
       "falcon-7b-lang-1    0.562677  0.527273              0.325670   \n",
       "falcon-7b-lang-10   0.515080  0.521753              0.352490   \n",
       "falcon-7b-lang-2    0.511781  0.524026              0.344828   \n",
       "falcon-7b-lang-20   0.510368  0.509091              0.302682   \n",
       "falcon-7b-lang-3    0.521678  0.542857              0.379310   \n",
       "falcon-7b-lang-45   0.518379  0.497078              0.283525   \n",
       "falcon-7b-lang-5    0.509425  0.531494              0.333333   \n",
       "\n",
       "task                sparta_qa_2reasoning  step_game_basic  step_game_hard  \\\n",
       "model                                                                       \n",
       "falcon-40b-lang-1               0.327189            0.237           0.113   \n",
       "falcon-40b-lang-10              0.373272            0.229           0.114   \n",
       "falcon-40b-lang-45              0.322581            0.224           0.111   \n",
       "falcon-7b-lang-1                0.336406            0.170           0.086   \n",
       "falcon-7b-lang-10               0.331797            0.150           0.104   \n",
       "falcon-7b-lang-2                0.368664            0.158           0.099   \n",
       "falcon-7b-lang-20               0.359447            0.150           0.108   \n",
       "falcon-7b-lang-3                0.331797            0.147           0.105   \n",
       "falcon-7b-lang-45               0.327189            0.138           0.098   \n",
       "falcon-7b-lang-5                0.345622            0.155           0.115   \n",
       "\n",
       "task                timedial  \n",
       "model                         \n",
       "falcon-40b-lang-1   0.269710  \n",
       "falcon-40b-lang-10  0.494467  \n",
       "falcon-40b-lang-45  0.251037  \n",
       "falcon-7b-lang-1    0.331950  \n",
       "falcon-7b-lang-10   0.822960  \n",
       "falcon-7b-lang-2    0.483402  \n",
       "falcon-7b-lang-20   0.159751  \n",
       "falcon-7b-lang-3    0.878976  \n",
       "falcon-7b-lang-45   0.271093  \n",
       "falcon-7b-lang-5    0.470954  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = pd.DataFrame(acc_data)\n",
    "acc_df.to_csv('chatgpt_eval_metric_final.csv', index=False)\n",
    "acc_df.pivot(index='model', columns='task', values='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335153e9-f493-486b-8bb4-08780d52291d",
   "metadata": {},
   "source": [
    "### Group per Reasoning Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "05052773-b610-4c8c-9c67-1944714d808a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_to_reasoning_type = {\n",
    "    'babi15': 'Deductive',\n",
    "    'babi16': 'Inductive',\n",
    "    'clutrr': 'Inductive',\n",
    "    'step_game_basic': 'Spatial',\n",
    "    'step_game_hard': 'Spatial',\n",
    "    'pep_3k': 'Commonsense',\n",
    "    'alpha_nli': 'Abductive',\n",
    "    'timedial': 'Temporal',\n",
    "    'sparta_qa_1reasoning': 'Spatial',\n",
    "    'sparta_qa_2reasoning': 'Spatial',\n",
    "    'commonsenseqa': 'Commonsense',\n",
    "    'ecare': 'Causal',\n",
    "    'covid_fact_scientific': 'Fact Checking',\n",
    "    'covid_fact_social': 'Fact Checking',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "00e7d069-1da7-46a8-8512-42158a51194f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasoning_data = {}\n",
    "\n",
    "# Iterate over tasks\n",
    "for task, model_data  in pred_data.items():\n",
    "    reasoning_type = task_to_reasoning_type[task]\n",
    "    # Iterate over models\n",
    "    for model, lang_data in model_data.items():\n",
    "        # Iterate over n_languages\n",
    "        for n_lang, preds in lang_data.items():\n",
    "            if reasoning_type not in reasoning_data:\n",
    "                reasoning_data[reasoning_type] = {}\n",
    "            if model not in reasoning_data[reasoning_type]:\n",
    "                reasoning_data[reasoning_type][model] = {}\n",
    "            if n_lang not in reasoning_data[reasoning_type][model]:\n",
    "                reasoning_data[reasoning_type][model][n_lang] = []\n",
    "            reasoning_data[reasoning_type][model][n_lang] += preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d0a62-2a04-4338-be3c-b127f245d432",
   "metadata": {},
   "source": [
    "### Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9e079b42-8365-49f3-9544-d4e9f4e807e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_data = {'reasoning_type': [], 'model': [], 'probability': []}\n",
    "# Iterate over tasks\n",
    "for reasoning, model_data  in reasoning_data.items():\n",
    "    # Iterate over models\n",
    "    for model, lang_data in model_data.items():\n",
    "        # Compute\n",
    "        lang1, pred1 = 1, lang_data[1]\n",
    "        lang_keys = list(lang_data.keys())\n",
    "        for i in range(len(lang_keys)):\n",
    "            lang2, pred2 = lang_keys[i], lang_data[lang_keys[i]]\n",
    "            if lang1 == lang2:\n",
    "                continue\n",
    "            proba = compute_statistics(pred1, pred2)\n",
    "            stats_data['reasoning_type'].append(reasoning)\n",
    "            stats_data['model'].append(f'{model}-lang-{lang2}')\n",
    "            stats_data['probability'].append(proba)\n",
    "stats_df = pd.DataFrame(stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4089a7b1-197a-4757-b02b-41bad53ad421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df.to_csv('significance/chatgpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cea7324d-dd8a-4026-adc8-d885409faa90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reasoning_type</th>\n",
       "      <th>model</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reasoning_type               model  probability\n",
       "0         Spatial    falcon-7b-lang-5        0.262\n",
       "1         Spatial   falcon-7b-lang-10        0.479\n",
       "2         Spatial   falcon-7b-lang-45        0.117\n",
       "3         Spatial   falcon-7b-lang-20        0.525\n",
       "4         Spatial    falcon-7b-lang-3        0.383\n",
       "5         Spatial    falcon-7b-lang-2        0.347\n",
       "6         Spatial  falcon-40b-lang-45        0.294\n",
       "7         Spatial  falcon-40b-lang-10        0.495\n",
       "8        Temporal    falcon-7b-lang-5        0.000\n",
       "9        Temporal   falcon-7b-lang-20        0.000\n",
       "10       Temporal   falcon-7b-lang-45        0.000\n",
       "11       Temporal   falcon-7b-lang-10        0.000\n",
       "12       Temporal    falcon-7b-lang-3        0.000\n",
       "13       Temporal    falcon-7b-lang-2        0.000\n",
       "14       Temporal  falcon-40b-lang-10        0.000\n",
       "15       Temporal  falcon-40b-lang-45        0.143\n",
       "16    Commonsense   falcon-7b-lang-45        0.000\n",
       "17    Commonsense   falcon-7b-lang-20        0.002\n",
       "18    Commonsense    falcon-7b-lang-2        0.199\n",
       "19    Commonsense    falcon-7b-lang-3        0.438\n",
       "20    Commonsense   falcon-7b-lang-10        0.047\n",
       "21    Commonsense    falcon-7b-lang-5        0.133\n",
       "22    Commonsense  falcon-40b-lang-10        0.000\n",
       "23    Commonsense  falcon-40b-lang-45        0.000\n",
       "24      Deductive   falcon-7b-lang-20        0.159\n",
       "25      Deductive   falcon-7b-lang-45        0.117\n",
       "26      Deductive    falcon-7b-lang-3        0.056\n",
       "27      Deductive   falcon-7b-lang-10        0.338\n",
       "28      Deductive    falcon-7b-lang-5        0.445\n",
       "29      Deductive    falcon-7b-lang-2        0.000\n",
       "30      Deductive  falcon-40b-lang-10        0.457\n",
       "31      Deductive  falcon-40b-lang-45        0.050\n",
       "32      Inductive   falcon-7b-lang-20        0.012\n",
       "33      Inductive   falcon-7b-lang-45        0.002\n",
       "34      Inductive    falcon-7b-lang-3        0.235\n",
       "35      Inductive   falcon-7b-lang-10        0.324\n",
       "36      Inductive    falcon-7b-lang-2        0.481\n",
       "37      Inductive    falcon-7b-lang-5        0.050\n",
       "38      Inductive  falcon-40b-lang-10        0.000\n",
       "39      Inductive  falcon-40b-lang-45        0.000\n",
       "40      Abductive   falcon-7b-lang-20        0.000\n",
       "41      Abductive   falcon-7b-lang-45        0.000\n",
       "42      Abductive    falcon-7b-lang-3        0.000\n",
       "43      Abductive   falcon-7b-lang-10        0.000\n",
       "44      Abductive    falcon-7b-lang-2        0.000\n",
       "45      Abductive    falcon-7b-lang-5        0.000\n",
       "46      Abductive  falcon-40b-lang-10        0.000\n",
       "47      Abductive  falcon-40b-lang-45        0.000\n",
       "48         Causal   falcon-7b-lang-20        0.000\n",
       "49         Causal   falcon-7b-lang-45        0.001\n",
       "50         Causal    falcon-7b-lang-3        0.004\n",
       "51         Causal   falcon-7b-lang-10        0.002\n",
       "52         Causal    falcon-7b-lang-2        0.001\n",
       "53         Causal    falcon-7b-lang-5        0.001\n",
       "54         Causal  falcon-40b-lang-10        0.062\n",
       "55         Causal  falcon-40b-lang-45        0.000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad860fb2-d282-4491-8b03-042e60c4b6a7",
   "metadata": {},
   "source": [
    "# MMLU Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19da804c-c926-433f-b50c-874599ef827a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mmlu_cat_df = pd.read_csv('mmlu_category.csv')\n",
    "mmlu_cat_df = mmlu_cat_df.set_index('subject')\n",
    "\n",
    "model_to_index = {\n",
    "    'falcon-40b': 0,\n",
    "    'falcon-40b-lang-1': 1,\n",
    "    'falcon-40b-lang-10': 2,\n",
    "    'falcon-40b-lang-45': 3,\n",
    "    'falcon-7b': 4,\n",
    "    'falcon-7b-lang-1': 5,\n",
    "    'falcon-7b-lang-2': 6,\n",
    "    'falcon-7b-lang-3': 7,\n",
    "    'falcon-7b-lang-5': 8,\n",
    "    'falcon-7b-lang-10': 9,\n",
    "    'falcon-7b-lang-20': 10,\n",
    "    'falcon-7b-lang-45': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bac65288-51b6-461c-8c61-1dec748b353e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "category_preds_data, level_preds_data = {}, {}\n",
    "for path in glob('results/results_falcon-*/*.csv'):\n",
    "    if 'baseline' in path:\n",
    "        continue\n",
    "    model = path.split('results_')[1].split('/')[0]\n",
    "    subject = path.split('/')[-1][:-4]\n",
    "    category = mmlu_cat_df.loc[subject, 'category']\n",
    "    model, n_lang = model.split('-lang-')\n",
    "    \n",
    "    # Get Level\n",
    "    if 'high_school' in path:\n",
    "        level = 'high_school'\n",
    "    elif 'college' in path:\n",
    "        level = 'college'\n",
    "    elif 'professional' in path:\n",
    "        level = 'professional'\n",
    "    elif 'elementary' in path:\n",
    "        level = 'elementary'\n",
    "    else:\n",
    "        level = 'other'\n",
    "        \n",
    "    # Compute Correctnesss & Accuracy\n",
    "    df = pd.read_csv(path)\n",
    "    num_correct = df.iloc[:,6].sum() \n",
    "    num_data = df.shape[0]\n",
    "    accuracy = num_correct / num_data\n",
    "    \n",
    "    if category not in category_preds_data:\n",
    "        category_preds_data[category] = {}\n",
    "    if level not in level_preds_data:\n",
    "        level_preds_data[level] = {}\n",
    "    \n",
    "    if model not in category_preds_data[category]:\n",
    "        category_preds_data[category][model] = {}\n",
    "    if model not in level_preds_data[level]:\n",
    "        level_preds_data[level][model] = {}\n",
    "        \n",
    "    if int(n_lang) not in category_preds_data[category][model]:\n",
    "        category_preds_data[category][model][int(n_lang)] = []\n",
    "    if int(n_lang) not in level_preds_data[level][model]:\n",
    "        level_preds_data[level][model][int(n_lang)] = []\n",
    "\n",
    "    category_preds_data[category][model][int(n_lang)] += df.iloc[:,6].tolist()\n",
    "    level_preds_data[level][model][int(n_lang)] += df.iloc[:,6].tolist()\n",
    "    # preds_data.append('')\n",
    "    # data.append({\n",
    "    #     'model': model, 'model_index': model_index, 'level': level, 'subject': subject, 'category': category, \n",
    "    #     'num_correct': num_correct, 'num_data': num_data, 'accuracy': accuracy * 100\n",
    "    # })\n",
    "# df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6d414-f51b-41e1-a553-274244fe552d",
   "metadata": {},
   "source": [
    "### Statistical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65041b96-354f-4e95-9e89-950ae2c7ca3c",
   "metadata": {},
   "source": [
    "##### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4baf487-1039-40d1-b6fa-5f5f5f174e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_stats_data = {'category': [], 'model': [], 'probability': []}\n",
    "# Iterate over tasks\n",
    "for category, model_data  in category_preds_data.items():\n",
    "    # Iterate over models\n",
    "    for model, lang_data in model_data.items():\n",
    "        # Compute\n",
    "        lang1, pred1 = 1, lang_data[1]\n",
    "        lang_keys = list(lang_data.keys())\n",
    "        for i in range(len(lang_keys)):\n",
    "            lang2, pred2 = lang_keys[i], lang_data[lang_keys[i]]\n",
    "            # if lang1 == lang2:\n",
    "            #     continue\n",
    "            proba = compute_statistics(pred1, pred2)\n",
    "            category_stats_data['category'].append(category)\n",
    "            category_stats_data['model'].append(f'{model}-lang-{lang2}')\n",
    "            category_stats_data['probability'].append(proba)\n",
    "category_stats_df = pd.DataFrame(category_stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cd2918b-251c-43cf-850f-724268ecbf10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_stats_df.to_csv('significance/category_mmlu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96fcd139-0082-472e-b506-6d0f1ceb6c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>model</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>sciences</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category               model  probability\n",
       "0              stem    falcon-7b-lang-3        0.225\n",
       "1              stem   falcon-7b-lang-20        0.033\n",
       "2              stem   falcon-7b-lang-45        0.103\n",
       "3              stem    falcon-7b-lang-5        0.450\n",
       "4              stem    falcon-7b-lang-1        0.493\n",
       "5              stem    falcon-7b-lang-2        0.396\n",
       "6              stem   falcon-7b-lang-10        0.004\n",
       "7              stem   falcon-40b-lang-1        0.500\n",
       "8              stem  falcon-40b-lang-10        0.078\n",
       "9              stem  falcon-40b-lang-45        0.027\n",
       "10            other    falcon-7b-lang-3        0.276\n",
       "11            other   falcon-7b-lang-20        0.252\n",
       "12            other   falcon-7b-lang-45        0.466\n",
       "13            other    falcon-7b-lang-5        0.341\n",
       "14            other    falcon-7b-lang-1        0.490\n",
       "15            other    falcon-7b-lang-2        0.227\n",
       "16            other   falcon-7b-lang-10        0.369\n",
       "17            other   falcon-40b-lang-1        0.515\n",
       "18            other  falcon-40b-lang-10        0.280\n",
       "19            other  falcon-40b-lang-45        0.043\n",
       "20         sciences    falcon-7b-lang-3        0.496\n",
       "21         sciences   falcon-7b-lang-20        0.086\n",
       "22         sciences   falcon-7b-lang-45        0.510\n",
       "23         sciences    falcon-7b-lang-5        0.320\n",
       "24         sciences    falcon-7b-lang-1        0.555\n",
       "25         sciences    falcon-7b-lang-2        0.502\n",
       "26         sciences   falcon-7b-lang-10        0.553\n",
       "27         sciences   falcon-40b-lang-1        0.544\n",
       "28         sciences  falcon-40b-lang-10        0.495\n",
       "29         sciences  falcon-40b-lang-45        0.508\n",
       "30       humanities    falcon-7b-lang-3        0.263\n",
       "31       humanities   falcon-7b-lang-20        0.362\n",
       "32       humanities   falcon-7b-lang-45        0.033\n",
       "33       humanities    falcon-7b-lang-5        0.101\n",
       "34       humanities    falcon-7b-lang-1        0.498\n",
       "35       humanities    falcon-7b-lang-2        0.490\n",
       "36       humanities   falcon-7b-lang-10        0.424\n",
       "37       humanities   falcon-40b-lang-1        0.542\n",
       "38       humanities  falcon-40b-lang-10        0.385\n",
       "39       humanities  falcon-40b-lang-45        0.001\n",
       "40  social_sciences    falcon-7b-lang-3        0.424\n",
       "41  social_sciences   falcon-7b-lang-20        0.026\n",
       "42  social_sciences   falcon-7b-lang-45        0.283\n",
       "43  social_sciences    falcon-7b-lang-5        0.306\n",
       "44  social_sciences    falcon-7b-lang-1        0.479\n",
       "45  social_sciences    falcon-7b-lang-2        0.320\n",
       "46  social_sciences   falcon-7b-lang-10        0.238\n",
       "47  social_sciences   falcon-40b-lang-1        0.502\n",
       "48  social_sciences  falcon-40b-lang-10        0.046\n",
       "49  social_sciences  falcon-40b-lang-45        0.000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05475b78-31d9-4dd8-aaa4-2fcc3b6b3f7e",
   "metadata": {},
   "source": [
    "##### Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb1dc5e0-0eed-40d3-8652-7e9f730953c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "level_stats_data = {'level': [], 'model': [], 'probability': []}\n",
    "# Iterate over tasks\n",
    "for level, model_data  in level_preds_data.items():\n",
    "    # Iterate over models\n",
    "    for model, lang_data in model_data.items():\n",
    "        # Compute\n",
    "        lang1, pred1 = 1, lang_data[1]\n",
    "        lang_keys = list(lang_data.keys())\n",
    "        for i in range(len(lang_keys)):\n",
    "            lang2, pred2 = lang_keys[i], lang_data[lang_keys[i]]\n",
    "            # if lang1 == lang2:\n",
    "            #     continue\n",
    "            proba = compute_statistics(pred1, pred2)\n",
    "            level_stats_data['level'].append(level)\n",
    "            level_stats_data['model'].append(f'{model}-lang-{lang2}')\n",
    "            level_stats_data['probability'].append(proba)\n",
    "level_stats_df = pd.DataFrame(level_stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a76434f2-66c5-481a-ac53-ed121d459dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "level_stats_df.to_csv('significance/level_mmlu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d381119-ec1e-4182-adb1-fe315370e75c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>model</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           level               model  probability\n",
       "0          other    falcon-7b-lang-3        0.191\n",
       "1          other   falcon-7b-lang-20        0.308\n",
       "2          other   falcon-7b-lang-45        0.080\n",
       "3          other    falcon-7b-lang-5        0.196\n",
       "4          other    falcon-7b-lang-1        0.518\n",
       "5          other    falcon-7b-lang-2        0.172\n",
       "6          other   falcon-7b-lang-10        0.386\n",
       "7          other   falcon-40b-lang-1        0.528\n",
       "8          other  falcon-40b-lang-10        0.291\n",
       "9          other  falcon-40b-lang-45        0.000\n",
       "10       college    falcon-7b-lang-3        0.358\n",
       "11       college   falcon-7b-lang-20        0.302\n",
       "12       college   falcon-7b-lang-45        0.084\n",
       "13       college    falcon-7b-lang-5        0.395\n",
       "14       college    falcon-7b-lang-1        0.503\n",
       "15       college    falcon-7b-lang-2        0.501\n",
       "16       college   falcon-7b-lang-10        0.179\n",
       "17       college   falcon-40b-lang-1        0.512\n",
       "18       college  falcon-40b-lang-10        0.102\n",
       "19       college  falcon-40b-lang-45        0.139\n",
       "20    elementary    falcon-7b-lang-3        0.384\n",
       "21    elementary   falcon-7b-lang-20        0.528\n",
       "22    elementary   falcon-7b-lang-45        0.058\n",
       "23    elementary    falcon-7b-lang-5        0.447\n",
       "24    elementary    falcon-7b-lang-1        0.531\n",
       "25    elementary    falcon-7b-lang-2        0.449\n",
       "26    elementary   falcon-7b-lang-10        0.050\n",
       "27    elementary   falcon-40b-lang-1        0.538\n",
       "28    elementary  falcon-40b-lang-10        0.350\n",
       "29    elementary  falcon-40b-lang-45        0.129\n",
       "30   high_school    falcon-7b-lang-3        0.138\n",
       "31   high_school   falcon-7b-lang-20        0.002\n",
       "32   high_school   falcon-7b-lang-45        0.327\n",
       "33   high_school    falcon-7b-lang-5        0.323\n",
       "34   high_school    falcon-7b-lang-1        0.516\n",
       "35   high_school    falcon-7b-lang-2        0.208\n",
       "36   high_school   falcon-7b-lang-10        0.147\n",
       "37   high_school   falcon-40b-lang-1        0.507\n",
       "38   high_school  falcon-40b-lang-10        0.055\n",
       "39   high_school  falcon-40b-lang-45        0.000\n",
       "40  professional    falcon-7b-lang-3        0.380\n",
       "41  professional   falcon-7b-lang-20        0.481\n",
       "42  professional   falcon-7b-lang-45        0.467\n",
       "43  professional    falcon-7b-lang-5        0.285\n",
       "44  professional    falcon-7b-lang-1        0.480\n",
       "45  professional    falcon-7b-lang-2        0.184\n",
       "46  professional   falcon-7b-lang-10        0.201\n",
       "47  professional   falcon-40b-lang-1        0.486\n",
       "48  professional  falcon-40b-lang-10        0.297\n",
       "49  professional  falcon-40b-lang-45        0.053"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175d386-8b51-45ec-b7c2-b2fdef8ac526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
