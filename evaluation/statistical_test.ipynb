{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "667a0cb2-7018-4c03-a15a-622035124005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "072dac52-991f-4562-82d6-0de0565db138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_statistics(pred1, pred2, n=10000):\n",
    "    acc1, acc2 = sum(pred1) / len(pred1), sum(pred2) / len(pred2)\n",
    "    ref_diff_acc = abs(acc1 - acc2)\n",
    "    preds = pred1 + pred2\n",
    "    \n",
    "    occurences = 0\n",
    "    for i in range(n):\n",
    "        pred_x, pred_y = train_test_split(preds, test_size=len(pred2), random_state=i)\n",
    "        acc_x, acc_y = sum(pred_x) / len(pred_x), sum(pred_y) / len(pred_y)\n",
    "        diff_acc = abs(acc_x - acc_y)\n",
    "        if diff_acc >= ref_diff_acc:\n",
    "            occurences += 1\n",
    "    \n",
    "    return occurences / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34efbe-b006-491f-b374-2fb9926f5296",
   "metadata": {},
   "source": [
    "# ChatGPT Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6cbe551-d4dc-4c5e-be41-4316f32fa919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for path in glob('outputs_chatgpt_eval/*.csv'):\n",
    "    task = path.split('/')[-1].split('_falcon-')[0]\n",
    "    model_name = 'falcon-' + path.split('/')[-1].split('_falcon-')[1][:-4]    \n",
    "    df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22c05f9-9453-4572-b614-cab423047638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_correct(row):\n",
    "    if str(row['Gold'])[0] == '[':\n",
    "        return str(row['Pred']) in map(lambda x: str(x), eval(row['Gold']))\n",
    "    else:\n",
    "        return str(row['Pred']) == str(row['Gold'])\n",
    "\n",
    "pred_data = {}\n",
    "acc_data = {'task': [], 'model': [], 'accuracy': []}\n",
    "for path in glob('outputs_chatgpt_eval/*.csv'):\n",
    "    task = path.split('/')[-1].split('_falcon-')[0]\n",
    "    model_name = 'falcon-' + path.split('/')[-1].split('_falcon-')[1][:-4]    \n",
    "    if ('lang' not in model_name) or ('covid' in task):\n",
    "        continue\n",
    "    model, n_lang = model_name.split('-lang-')\n",
    "        \n",
    "    df = pd.read_csv(path)\n",
    "    if task == 'ecare':\n",
    "        df['Gold'] = df['Gold'].apply(lambda x: 'A' if str(x) == '0' else 'B')\n",
    "    df['correct'] = df.apply(check_correct, axis='columns')\n",
    "    \n",
    "    if task not in pred_data:\n",
    "        pred_data[task] = {}    \n",
    "    if model not in pred_data[task]:\n",
    "        pred_data[task][model] = {}\n",
    "    pred_data[task][model][int(n_lang)] = df['correct'].tolist()\n",
    "    \n",
    "    acc_data['task'].append(task)\n",
    "    acc_data['model'].append(model_name)\n",
    "    acc_data['accuracy'].append(df['correct'].sum() / len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9aa6164-f829-4583-816f-0971e57f536f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>task</th>\n",
       "      <th>alpha_nli</th>\n",
       "      <th>babi15</th>\n",
       "      <th>babi16</th>\n",
       "      <th>clutrr</th>\n",
       "      <th>commonsenseqa</th>\n",
       "      <th>ecare</th>\n",
       "      <th>pep_3k</th>\n",
       "      <th>sparta_qa_1reasoning</th>\n",
       "      <th>sparta_qa_2reasoning</th>\n",
       "      <th>step_game_basic</th>\n",
       "      <th>step_game_hard</th>\n",
       "      <th>timedial</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>falcon-40b-lang-1</th>\n",
       "      <td>0.783589</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.621</td>\n",
       "      <td>0.296684</td>\n",
       "      <td>0.719902</td>\n",
       "      <td>0.662111</td>\n",
       "      <td>0.561039</td>\n",
       "      <td>0.417625</td>\n",
       "      <td>0.327189</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.269710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-40b-lang-10</th>\n",
       "      <td>0.735861</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.247818</td>\n",
       "      <td>0.525799</td>\n",
       "      <td>0.639491</td>\n",
       "      <td>0.535065</td>\n",
       "      <td>0.402299</td>\n",
       "      <td>0.373272</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.494467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-40b-lang-45</th>\n",
       "      <td>0.631906</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.243455</td>\n",
       "      <td>0.447993</td>\n",
       "      <td>0.614515</td>\n",
       "      <td>0.513312</td>\n",
       "      <td>0.421456</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.251037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-1</th>\n",
       "      <td>0.618830</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.199825</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>0.562677</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.325670</td>\n",
       "      <td>0.336406</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.331950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-10</th>\n",
       "      <td>0.525989</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.196335</td>\n",
       "      <td>0.209664</td>\n",
       "      <td>0.515080</td>\n",
       "      <td>0.521753</td>\n",
       "      <td>0.352490</td>\n",
       "      <td>0.331797</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.822960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-2</th>\n",
       "      <td>0.575678</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.198953</td>\n",
       "      <td>0.237510</td>\n",
       "      <td>0.511781</td>\n",
       "      <td>0.524026</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.368664</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.483402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-20</th>\n",
       "      <td>0.517489</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.158813</td>\n",
       "      <td>0.187551</td>\n",
       "      <td>0.510368</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.302682</td>\n",
       "      <td>0.359447</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.159751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-3</th>\n",
       "      <td>0.516509</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.185864</td>\n",
       "      <td>0.213759</td>\n",
       "      <td>0.521678</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.331797</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.878976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-45</th>\n",
       "      <td>0.520105</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.121291</td>\n",
       "      <td>0.192465</td>\n",
       "      <td>0.518379</td>\n",
       "      <td>0.497078</td>\n",
       "      <td>0.283525</td>\n",
       "      <td>0.327189</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.271093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-7b-lang-5</th>\n",
       "      <td>0.519124</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.169284</td>\n",
       "      <td>0.206388</td>\n",
       "      <td>0.509425</td>\n",
       "      <td>0.531494</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.345622</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.470954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "task                alpha_nli  babi15  babi16    clutrr  commonsenseqa  \\\n",
       "model                                                                    \n",
       "falcon-40b-lang-1    0.783589   0.224   0.621  0.296684       0.719902   \n",
       "falcon-40b-lang-10   0.735861   0.227   0.504  0.247818       0.525799   \n",
       "falcon-40b-lang-45   0.631906   0.259   0.549  0.243455       0.447993   \n",
       "falcon-7b-lang-1     0.618830   0.207   0.518  0.199825       0.260442   \n",
       "falcon-7b-lang-10    0.525989   0.216   0.508  0.196335       0.209664   \n",
       "falcon-7b-lang-2     0.575678   0.302   0.518  0.198953       0.237510   \n",
       "falcon-7b-lang-20    0.517489   0.227   0.491  0.158813       0.187551   \n",
       "falcon-7b-lang-3     0.516509   0.240   0.511  0.185864       0.213759   \n",
       "falcon-7b-lang-45    0.520105   0.231   0.517  0.121291       0.192465   \n",
       "falcon-7b-lang-5     0.519124   0.211   0.496  0.169284       0.206388   \n",
       "\n",
       "task                   ecare    pep_3k  sparta_qa_1reasoning  \\\n",
       "model                                                          \n",
       "falcon-40b-lang-1   0.662111  0.561039              0.417625   \n",
       "falcon-40b-lang-10  0.639491  0.535065              0.402299   \n",
       "falcon-40b-lang-45  0.614515  0.513312              0.421456   \n",
       "falcon-7b-lang-1    0.562677  0.527273              0.325670   \n",
       "falcon-7b-lang-10   0.515080  0.521753              0.352490   \n",
       "falcon-7b-lang-2    0.511781  0.524026              0.344828   \n",
       "falcon-7b-lang-20   0.510368  0.509091              0.302682   \n",
       "falcon-7b-lang-3    0.521678  0.542857              0.379310   \n",
       "falcon-7b-lang-45   0.518379  0.497078              0.283525   \n",
       "falcon-7b-lang-5    0.509425  0.531494              0.333333   \n",
       "\n",
       "task                sparta_qa_2reasoning  step_game_basic  step_game_hard  \\\n",
       "model                                                                       \n",
       "falcon-40b-lang-1               0.327189            0.237           0.113   \n",
       "falcon-40b-lang-10              0.373272            0.229           0.114   \n",
       "falcon-40b-lang-45              0.322581            0.224           0.111   \n",
       "falcon-7b-lang-1                0.336406            0.170           0.086   \n",
       "falcon-7b-lang-10               0.331797            0.150           0.104   \n",
       "falcon-7b-lang-2                0.368664            0.158           0.099   \n",
       "falcon-7b-lang-20               0.359447            0.150           0.108   \n",
       "falcon-7b-lang-3                0.331797            0.147           0.105   \n",
       "falcon-7b-lang-45               0.327189            0.138           0.098   \n",
       "falcon-7b-lang-5                0.345622            0.155           0.115   \n",
       "\n",
       "task                timedial  \n",
       "model                         \n",
       "falcon-40b-lang-1   0.269710  \n",
       "falcon-40b-lang-10  0.494467  \n",
       "falcon-40b-lang-45  0.251037  \n",
       "falcon-7b-lang-1    0.331950  \n",
       "falcon-7b-lang-10   0.822960  \n",
       "falcon-7b-lang-2    0.483402  \n",
       "falcon-7b-lang-20   0.159751  \n",
       "falcon-7b-lang-3    0.878976  \n",
       "falcon-7b-lang-45   0.271093  \n",
       "falcon-7b-lang-5    0.470954  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = pd.DataFrame(acc_data)\n",
    "acc_df.to_csv('chatgpt_eval_metric_final.csv', index=False)\n",
    "acc_df.pivot(index='model', columns='task', values='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335153e9-f493-486b-8bb4-08780d52291d",
   "metadata": {},
   "source": [
    "### Group per Reasoning Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05052773-b610-4c8c-9c67-1944714d808a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_to_reasoning_type = {\n",
    "    'babi15': 'Deductive',\n",
    "    'babi16': 'Inductive',\n",
    "    'clutrr': 'Inductive',\n",
    "    'step_game_basic': 'Spatial',\n",
    "    'step_game_hard': 'Spatial',\n",
    "    'pep_3k': 'Commonsense',\n",
    "    'alpha_nli': 'Abductive',\n",
    "    'timedial': 'Temporal',\n",
    "    'sparta_qa_1reasoning': 'Spatial',\n",
    "    'sparta_qa_2reasoning': 'Spatial',\n",
    "    'commonsenseqa': 'Commonsense',\n",
    "    'ecare': 'Causal',\n",
    "    'covid_fact_scientific': 'Fact Checking',\n",
    "    'covid_fact_social': 'Fact Checking',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e7d069-1da7-46a8-8512-42158a51194f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reasoning_data = {}\n",
    "\n",
    "# Iterate over tasks\n",
    "for task, model_data  in pred_data.items():\n",
    "    reasoning_type = task_to_reasoning_type[task]\n",
    "    # Iterate over models\n",
    "    for model, lang_data in model_data.items():\n",
    "        # Iterate over n_languages\n",
    "        for n_lang, preds in lang_data.items():\n",
    "            if reasoning_type not in reasoning_data:\n",
    "                reasoning_data[reasoning_type] = {}\n",
    "            if model not in reasoning_data[reasoning_type]:\n",
    "                reasoning_data[reasoning_type][model] = {}\n",
    "            if n_lang not in reasoning_data[reasoning_type][model]:\n",
    "                reasoning_data[reasoning_type][model][n_lang] = []\n",
    "            reasoning_data[reasoning_type][model][n_lang] += preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d0a62-2a04-4338-be3c-b127f245d432",
   "metadata": {},
   "source": [
    "### Statistical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e079b42-8365-49f3-9544-d4e9f4e807e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_data = {'reasoning_type': [], 'model': [], 'probability': []}\n",
    "# Iterate over tasks\n",
    "for reasoning, model_data  in reasoning_data.items():\n",
    "    # Iterate over models\n",
    "    for model, lang_data in model_data.items():\n",
    "        # Compute\n",
    "        lang1, pred1 = 1, lang_data[1]\n",
    "        lang_keys = list(lang_data.keys())\n",
    "        for i in range(len(lang_keys)):\n",
    "            lang2, pred2 = lang_keys[i], lang_data[lang_keys[i]]\n",
    "            if lang1 == lang2:\n",
    "                continue\n",
    "            proba = compute_statistics(pred1, pred2)\n",
    "            stats_data['reasoning_type'].append(reasoning)\n",
    "            stats_data['model'].append(f'{model}-lang-{lang2}')\n",
    "            stats_data['probability'].append(proba)\n",
    "stats_df = pd.DataFrame(stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a9f54e-627b-4da4-9016-8b105055792e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reasoning_type</th>\n",
       "      <th>model</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.5279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.9129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.2178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.7687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.6499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.6268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Spatial</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Temporal</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.2690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.0022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.4255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.8619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.2644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.3003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.2137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.0869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.6627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.8709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.9144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Deductive</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.0039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.4783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.6640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.0735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Inductive</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Abductive</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.0019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.0011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.1324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Causal</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reasoning_type               model  probability\n",
       "0         Spatial    falcon-7b-lang-5       0.5279\n",
       "1         Spatial   falcon-7b-lang-10       0.9129\n",
       "2         Spatial   falcon-7b-lang-45       0.2178\n",
       "3         Spatial   falcon-7b-lang-20       1.0000\n",
       "4         Spatial    falcon-7b-lang-3       0.7687\n",
       "5         Spatial    falcon-7b-lang-2       0.6499\n",
       "6         Spatial  falcon-40b-lang-45       0.6268\n",
       "7         Spatial  falcon-40b-lang-10       1.0000\n",
       "8        Temporal    falcon-7b-lang-5       0.0000\n",
       "9        Temporal   falcon-7b-lang-20       0.0000\n",
       "10       Temporal   falcon-7b-lang-45       0.0005\n",
       "11       Temporal   falcon-7b-lang-10       0.0000\n",
       "12       Temporal    falcon-7b-lang-3       0.0000\n",
       "13       Temporal    falcon-7b-lang-2       0.0000\n",
       "14       Temporal  falcon-40b-lang-10       0.0000\n",
       "15       Temporal  falcon-40b-lang-45       0.2690\n",
       "16    Commonsense   falcon-7b-lang-45       0.0001\n",
       "17    Commonsense   falcon-7b-lang-20       0.0022\n",
       "18    Commonsense    falcon-7b-lang-2       0.4255\n",
       "19    Commonsense    falcon-7b-lang-3       0.8619\n",
       "20    Commonsense   falcon-7b-lang-10       0.0935\n",
       "21    Commonsense    falcon-7b-lang-5       0.2644\n",
       "22    Commonsense  falcon-40b-lang-10       0.0000\n",
       "23    Commonsense  falcon-40b-lang-45       0.0000\n",
       "24      Deductive   falcon-7b-lang-20       0.3003\n",
       "25      Deductive   falcon-7b-lang-45       0.2137\n",
       "26      Deductive    falcon-7b-lang-3       0.0869\n",
       "27      Deductive   falcon-7b-lang-10       0.6627\n",
       "28      Deductive    falcon-7b-lang-5       0.8709\n",
       "29      Deductive    falcon-7b-lang-2       0.0000\n",
       "30      Deductive  falcon-40b-lang-10       0.9144\n",
       "31      Deductive  falcon-40b-lang-45       0.0724\n",
       "32      Inductive   falcon-7b-lang-20       0.0160\n",
       "33      Inductive   falcon-7b-lang-45       0.0039\n",
       "34      Inductive    falcon-7b-lang-3       0.4783\n",
       "35      Inductive   falcon-7b-lang-10       0.6640\n",
       "36      Inductive    falcon-7b-lang-2       1.0000\n",
       "37      Inductive    falcon-7b-lang-5       0.0735\n",
       "38      Inductive  falcon-40b-lang-10       0.0000\n",
       "39      Inductive  falcon-40b-lang-45       0.0000\n",
       "40      Abductive   falcon-7b-lang-20       0.0000\n",
       "41      Abductive   falcon-7b-lang-45       0.0000\n",
       "42      Abductive    falcon-7b-lang-3       0.0000\n",
       "43      Abductive   falcon-7b-lang-10       0.0000\n",
       "44      Abductive    falcon-7b-lang-2       0.0005\n",
       "45      Abductive    falcon-7b-lang-5       0.0000\n",
       "46      Abductive  falcon-40b-lang-10       0.0000\n",
       "47      Abductive  falcon-40b-lang-45       0.0000\n",
       "48         Causal   falcon-7b-lang-20       0.0008\n",
       "49         Causal   falcon-7b-lang-45       0.0037\n",
       "50         Causal    falcon-7b-lang-3       0.0066\n",
       "51         Causal   falcon-7b-lang-10       0.0019\n",
       "52         Causal    falcon-7b-lang-2       0.0011\n",
       "53         Causal    falcon-7b-lang-5       0.0004\n",
       "54         Causal  falcon-40b-lang-10       0.1324\n",
       "55         Causal  falcon-40b-lang-45       0.0013"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4089a7b1-197a-4757-b02b-41bad53ad421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_df.to_csv('significance/chatgpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad860fb2-d282-4491-8b03-042e60c4b6a7",
   "metadata": {},
   "source": [
    "# MMLU Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19da804c-c926-433f-b50c-874599ef827a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mmlu_cat_df = pd.read_csv('mmlu_category.csv')\n",
    "mmlu_cat_df = mmlu_cat_df.set_index('subject')\n",
    "\n",
    "model_to_index = {\n",
    "    'falcon-40b': 0,\n",
    "    'falcon-40b-lang-1': 1,\n",
    "    'falcon-40b-lang-10': 2,\n",
    "    'falcon-40b-lang-45': 3,\n",
    "    'falcon-7b': 4,\n",
    "    'falcon-7b-lang-1': 5,\n",
    "    'falcon-7b-lang-2': 6,\n",
    "    'falcon-7b-lang-3': 7,\n",
    "    'falcon-7b-lang-5': 8,\n",
    "    'falcon-7b-lang-10': 9,\n",
    "    'falcon-7b-lang-20': 10,\n",
    "    'falcon-7b-lang-45': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bac65288-51b6-461c-8c61-1dec748b353e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "category_preds_data, level_preds_data = {}, {}\n",
    "for path in glob('results/results_falcon-*/*.csv'):\n",
    "    if 'baseline' in path:\n",
    "        continue\n",
    "    model = path.split('results_')[1].split('/')[0]\n",
    "    subject = path.split('/')[-1][:-4]\n",
    "    category = mmlu_cat_df.loc[subject, 'category']\n",
    "    model, n_lang = model.split('-lang-')\n",
    "    \n",
    "    # Get Level\n",
    "    if 'high_school' in path:\n",
    "        level = 'high_school'\n",
    "    elif 'college' in path:\n",
    "        level = 'college'\n",
    "    elif 'professional' in path:\n",
    "        level = 'professional'\n",
    "    elif 'elementary' in path:\n",
    "        level = 'elementary'\n",
    "    else:\n",
    "        level = 'other'\n",
    "        \n",
    "    # Compute Correctnesss & Accuracy\n",
    "    df = pd.read_csv(path)\n",
    "    num_correct = df.iloc[:,6].sum() \n",
    "    num_data = df.shape[0]\n",
    "    accuracy = num_correct / num_data\n",
    "    \n",
    "    if category not in category_preds_data:\n",
    "        category_preds_data[category] = {}\n",
    "    if level not in level_preds_data:\n",
    "        level_preds_data[level] = {}\n",
    "    \n",
    "    if model not in category_preds_data[category]:\n",
    "        category_preds_data[category][model] = {}\n",
    "    if model not in level_preds_data[level]:\n",
    "        level_preds_data[level][model] = {}\n",
    "        \n",
    "    if int(n_lang) not in category_preds_data[category][model]:\n",
    "        category_preds_data[category][model][int(n_lang)] = []\n",
    "    if int(n_lang) not in level_preds_data[level][model]:\n",
    "        level_preds_data[level][model][int(n_lang)] = []\n",
    "\n",
    "    category_preds_data[category][model][int(n_lang)] += df.iloc[:,6].tolist()\n",
    "    level_preds_data[level][model][int(n_lang)] += df.iloc[:,6].tolist()\n",
    "    # preds_data.append('')\n",
    "    # data.append({\n",
    "    #     'model': model, 'model_index': model_index, 'level': level, 'subject': subject, 'category': category, \n",
    "    #     'num_correct': num_correct, 'num_data': num_data, 'accuracy': accuracy * 100\n",
    "    # })\n",
    "# df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6d414-f51b-41e1-a553-274244fe552d",
   "metadata": {},
   "source": [
    "### Statistical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65041b96-354f-4e95-9e89-950ae2c7ca3c",
   "metadata": {},
   "source": [
    "##### Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4baf487-1039-40d1-b6fa-5f5f5f174e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_stats_data = {'category': [], 'model': [], 'probability': []}\n",
    "# Iterate over tasks\n",
    "for category, model_data  in category_preds_data.items():\n",
    "    # Iterate over models\n",
    "    for model, lang_data in model_data.items():\n",
    "        # Compute\n",
    "        lang1, pred1 = 1, lang_data[1]\n",
    "        lang_keys = list(lang_data.keys())\n",
    "        for i in range(len(lang_keys)):\n",
    "            lang2, pred2 = lang_keys[i], lang_data[lang_keys[i]]\n",
    "            # if lang1 == lang2:\n",
    "            #     continue\n",
    "            proba = compute_statistics(pred1, pred2)\n",
    "            category_stats_data['category'].append(category)\n",
    "            category_stats_data['model'].append(f'{model}-lang-{lang2}')\n",
    "            category_stats_data['probability'].append(proba)\n",
    "category_stats_df = pd.DataFrame(category_stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd2918b-251c-43cf-850f-724268ecbf10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_stats_df.to_csv('significance/category_mmlu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96fcd139-0082-472e-b506-6d0f1ceb6c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>model</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.4989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.0646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.2168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.9520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.8158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.1676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>stem</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.5225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.4910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.8654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.6592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.4401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.7325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.5644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.8332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.0856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.5803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.6853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.6484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.5259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.0851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>social_sciences</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.5311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.7108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.0655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.9629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.8133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.8094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>humanities</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category               model  probability\n",
       "0              stem    falcon-7b-lang-3       0.4989\n",
       "1              stem   falcon-7b-lang-20       0.0646\n",
       "2              stem   falcon-7b-lang-45       0.2168\n",
       "3              stem    falcon-7b-lang-5       0.9520\n",
       "4              stem    falcon-7b-lang-1       1.0000\n",
       "5              stem    falcon-7b-lang-2       0.8158\n",
       "6              stem   falcon-7b-lang-10       0.0050\n",
       "7              stem   falcon-40b-lang-1       1.0000\n",
       "8              stem  falcon-40b-lang-10       0.1676\n",
       "9              stem  falcon-40b-lang-45       0.0497\n",
       "10            other    falcon-7b-lang-3       0.5225\n",
       "11            other   falcon-7b-lang-20       0.4910\n",
       "12            other   falcon-7b-lang-45       0.8654\n",
       "13            other    falcon-7b-lang-5       0.6592\n",
       "14            other    falcon-7b-lang-1       1.0000\n",
       "15            other    falcon-7b-lang-2       0.4401\n",
       "16            other   falcon-7b-lang-10       0.7325\n",
       "17            other   falcon-40b-lang-1       1.0000\n",
       "18            other  falcon-40b-lang-10       0.5644\n",
       "19            other  falcon-40b-lang-45       0.0721\n",
       "20  social_sciences    falcon-7b-lang-3       0.8332\n",
       "21  social_sciences   falcon-7b-lang-20       0.0856\n",
       "22  social_sciences   falcon-7b-lang-45       0.5803\n",
       "23  social_sciences    falcon-7b-lang-5       0.6853\n",
       "24  social_sciences    falcon-7b-lang-1       1.0000\n",
       "25  social_sciences    falcon-7b-lang-2       0.6484\n",
       "26  social_sciences   falcon-7b-lang-10       0.5259\n",
       "27  social_sciences   falcon-40b-lang-1       1.0000\n",
       "28  social_sciences  falcon-40b-lang-10       0.0851\n",
       "29  social_sciences  falcon-40b-lang-45       0.0004\n",
       "30       humanities    falcon-7b-lang-3       0.5311\n",
       "31       humanities   falcon-7b-lang-20       0.7108\n",
       "32       humanities   falcon-7b-lang-45       0.0655\n",
       "33       humanities    falcon-7b-lang-5       0.1980\n",
       "34       humanities    falcon-7b-lang-1       1.0000\n",
       "35       humanities    falcon-7b-lang-2       0.9629\n",
       "36       humanities   falcon-7b-lang-10       0.8133\n",
       "37       humanities   falcon-40b-lang-1       1.0000\n",
       "38       humanities  falcon-40b-lang-10       0.8094\n",
       "39       humanities  falcon-40b-lang-45       0.0013"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05475b78-31d9-4dd8-aaa4-2fcc3b6b3f7e",
   "metadata": {},
   "source": [
    "##### Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb1dc5e0-0eed-40d3-8652-7e9f730953c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "level_stats_data = {'level': [], 'model': [], 'probability': []}\n",
    "# Iterate over tasks\n",
    "for level, model_data  in level_preds_data.items():\n",
    "    # Iterate over models\n",
    "    for model, lang_data in model_data.items():\n",
    "        # Compute\n",
    "        lang1, pred1 = 1, lang_data[1]\n",
    "        lang_keys = list(lang_data.keys())\n",
    "        for i in range(len(lang_keys)):\n",
    "            lang2, pred2 = lang_keys[i], lang_data[lang_keys[i]]\n",
    "            # if lang1 == lang2:\n",
    "            #     continue\n",
    "            proba = compute_statistics(pred1, pred2)\n",
    "            level_stats_data['level'].append(level)\n",
    "            level_stats_data['model'].append(f'{model}-lang-{lang2}')\n",
    "            level_stats_data['probability'].append(proba)\n",
    "level_stats_df = pd.DataFrame(level_stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a76434f2-66c5-481a-ac53-ed121d459dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "level_stats_df.to_csv('significance/level_mmlu.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d381119-ec1e-4182-adb1-fe315370e75c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>model</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.4163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.6316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.1805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.4001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.3076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.7269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.5727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>other</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.7111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.6276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.1541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.8021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.3928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.2157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>college</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.2857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.7927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.1467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.9304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.1048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.6882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>elementary</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.2343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.2929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.0036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.6167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.6547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.4285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.3112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.1120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>high_school</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-3</td>\n",
       "      <td>0.7331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-20</td>\n",
       "      <td>0.9038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-45</td>\n",
       "      <td>0.8976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-5</td>\n",
       "      <td>0.5505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-2</td>\n",
       "      <td>0.3887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-7b-lang-10</td>\n",
       "      <td>0.4343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-40b-lang-1</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-40b-lang-10</td>\n",
       "      <td>0.6301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>professional</td>\n",
       "      <td>falcon-40b-lang-45</td>\n",
       "      <td>0.1037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           level               model  probability\n",
       "0          other    falcon-7b-lang-3       0.4163\n",
       "1          other   falcon-7b-lang-20       0.6316\n",
       "2          other   falcon-7b-lang-45       0.1805\n",
       "3          other    falcon-7b-lang-5       0.4001\n",
       "4          other    falcon-7b-lang-1       1.0000\n",
       "5          other    falcon-7b-lang-2       0.3076\n",
       "6          other   falcon-7b-lang-10       0.7269\n",
       "7          other   falcon-40b-lang-1       1.0000\n",
       "8          other  falcon-40b-lang-10       0.5727\n",
       "9          other  falcon-40b-lang-45       0.0014\n",
       "10       college    falcon-7b-lang-3       0.7111\n",
       "11       college   falcon-7b-lang-20       0.6276\n",
       "12       college   falcon-7b-lang-45       0.1541\n",
       "13       college    falcon-7b-lang-5       0.8021\n",
       "14       college    falcon-7b-lang-1       1.0000\n",
       "15       college    falcon-7b-lang-2       1.0000\n",
       "16       college   falcon-7b-lang-10       0.3928\n",
       "17       college   falcon-40b-lang-1       1.0000\n",
       "18       college  falcon-40b-lang-10       0.2157\n",
       "19       college  falcon-40b-lang-45       0.2857\n",
       "20    elementary    falcon-7b-lang-3       0.7927\n",
       "21    elementary   falcon-7b-lang-20       1.0000\n",
       "22    elementary   falcon-7b-lang-45       0.1467\n",
       "23    elementary    falcon-7b-lang-5       0.9304\n",
       "24    elementary    falcon-7b-lang-1       1.0000\n",
       "25    elementary    falcon-7b-lang-2       0.9270\n",
       "26    elementary   falcon-7b-lang-10       0.1048\n",
       "27    elementary   falcon-40b-lang-1       1.0000\n",
       "28    elementary  falcon-40b-lang-10       0.6882\n",
       "29    elementary  falcon-40b-lang-45       0.2343\n",
       "30   high_school    falcon-7b-lang-3       0.2929\n",
       "31   high_school   falcon-7b-lang-20       0.0036\n",
       "32   high_school   falcon-7b-lang-45       0.6167\n",
       "33   high_school    falcon-7b-lang-5       0.6547\n",
       "34   high_school    falcon-7b-lang-1       1.0000\n",
       "35   high_school    falcon-7b-lang-2       0.4285\n",
       "36   high_school   falcon-7b-lang-10       0.3112\n",
       "37   high_school   falcon-40b-lang-1       1.0000\n",
       "38   high_school  falcon-40b-lang-10       0.1120\n",
       "39   high_school  falcon-40b-lang-45       0.0000\n",
       "40  professional    falcon-7b-lang-3       0.7331\n",
       "41  professional   falcon-7b-lang-20       0.9038\n",
       "42  professional   falcon-7b-lang-45       0.8976\n",
       "43  professional    falcon-7b-lang-5       0.5505\n",
       "44  professional    falcon-7b-lang-1       1.0000\n",
       "45  professional    falcon-7b-lang-2       0.3887\n",
       "46  professional   falcon-7b-lang-10       0.4343\n",
       "47  professional   falcon-40b-lang-1       1.0000\n",
       "48  professional  falcon-40b-lang-10       0.6301\n",
       "49  professional  falcon-40b-lang-45       0.1037"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175d386-8b51-45ec-b7c2-b2fdef8ac526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41b3fbec-b0a9-452d-bbf8-bc0eecdda240",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stem-other': 979, 'other-other': 2380, 'stem-college': 546, 'other-college': 173, 'social_sciences-other': 901, 'stem-elementary': 378, 'humanities-other': 2565, 'stem-high_school': 1250, 'humanities-high_school': 606, 'social_sciences-high_school': 1564, 'other-professional': 554, 'humanities-professional': 1534, 'social_sciences-professional': 612}\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "cat = {}\n",
    "for path in glob('results/results_falcon-40b-lang-1/*.csv'):\n",
    "    if 'baseline' in path:\n",
    "        continue\n",
    "    model = path.split('results_')[1].split('/')[0]\n",
    "    subject = path.split('/')[-1][:-4]\n",
    "    category = mmlu_cat_df.loc[subject, 'category']\n",
    "    model, n_lang = model.split('-lang-')\n",
    "    \n",
    "    # Get Level\n",
    "    if 'high_school' in path:\n",
    "        level = 'high_school'\n",
    "    elif 'college' in path:\n",
    "        level = 'college'\n",
    "    elif 'professional' in path:\n",
    "        level = 'professional'\n",
    "    elif 'elementary' in path:\n",
    "        level = 'elementary'\n",
    "    else:\n",
    "        level = 'other'\n",
    "        \n",
    "    # Compute Correctnesss & Accuracy\n",
    "    df = pd.read_csv(path)\n",
    "    num_correct = df.iloc[:,6].sum() \n",
    "    num_data = df.shape[0]\n",
    "    accuracy = num_correct / num_data\n",
    "    \n",
    "    if category + '-' + level not in cat:        \n",
    "        cat[category + '-' + level] = 0\n",
    "    cat[category + '-' + level] += num_data\n",
    "print(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ce6bd27-80a8-4fe3-af97-c30423c86ace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(171, 11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c58631-f109-48f9-830e-322ddb243270",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
