"""nusacrowd zero-shot prompt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru8DyS2ALWfRdkjOPHj-KNjw6Pfa44Nd
"""
import os, sys
import csv
from os.path import exists

from numpy import argmax, stack
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import classification_report, precision_recall_fscore_support

import torch
import torch.nn.functional as F

#from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, set_seed

from prompt_utils import get_prompt, get_label_mapping
from data_utils import load_nlu_datasets

#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp
#!pip install transformers
#!pip install sentencepiece

DEBUG=False

def to_prompt_copa(input, prompt, labels, prompt_lang):
    #dynamic prompt depending question type (cause vs effect)
    prompt = prompt[input['question']]
    prompt = prompt.replace('[PREMISE]', input['premise'])
    prompt = prompt.replace('[PREMISE_STRIP]', input['premise'].rstrip('.'))
    prompt = prompt.replace('[OPTION_1]', input['choice1'])
    prompt = prompt.replace('[OPTION_2]', input['choice2'])
    return prompt

def to_prompt_indo_story_cloze(input, prompt, labels, prompt_lang):
    prompt = prompt.replace('[PREMISE]', input['premise'])
    return prompt

def to_prompt_sst2(input, prompt, labels, prompt_lang):
    prompt = prompt.replace('[PREMISE]', input['sentence'])
    return prompt

def to_prompt_qnli(input, prompt, labels, prompt_lang):
    prompt = prompt.replace('[ANSWER]', input['text2'])
    prompt = prompt.replace('[QUESTION]', input['text1'])
    return prompt

def to_prompt_wnli(input, prompt, labels, prompt_lang):
    prompt = prompt.replace('[SENTENCE2]', input['text2'])
    prompt = prompt.replace('[SENTENCE1]', input['text1'])
    return prompt

def to_prompt_tweet_topic_single(input, prompt, labels, prompt_lang):
    prompt = prompt.replace('[TWEET]', input['text'])
    return prompt

@torch.inference_mode()
def get_logprobs(model, tokenizer, inputs, label_ids=None, label_attn=None):
    inputs = tokenizer(inputs, return_tensors="pt", padding=True, truncation=True, max_length=1024).to('cuda')    
    if model.config.is_encoder_decoder:
        label_ids = label_ids.repeat((inputs['input_ids'].shape[0],1))
        label_attn = label_attn.repeat((inputs['input_ids'].shape[0],1))
        logits = model(**inputs, labels=label_ids).logits
        logprobs = torch.gather(F.log_softmax(logits, dim=-1), 2, label_ids.unsqueeze(2)).squeeze(dim=-1) * label_attn # Info for decoder to generate label token(s)
        return logprobs.sum(dim=-1).cpu()
    else:
        logits = model(**inputs).logits
        output_ids = inputs["input_ids"][:, 1:]
        logprobs = torch.gather(F.log_softmax(logits, dim=-1), 2, output_ids.unsqueeze(2)).squeeze(dim=-1)
        logprobs[inputs["attention_mask"][:, :-1] == 0] = 0
        return logprobs.sum(dim=1).cpu()


@torch.inference_mode()
def predict_classification(model, tokenizer, prompts, labels):
    if model.config.is_encoder_decoder:
        labels_encoded = tokenizer(labels, add_special_tokens=False, padding=True, return_tensors='pt')
        list_label_ids = labels_encoded['input_ids'].to('cuda')
        list_label_attn = labels_encoded['attention_mask'].to('cuda')
        
        inputs = [prompt.replace('[LABELS_CHOICE]', '') for prompt in prompts]
        probs = []
        for (label_ids, label_attn) in zip(list_label_ids, list_label_attn):
            probs.append(
                get_logprobs(model, tokenizer, inputs, label_ids.view(1,-1), label_attn.view(1,-1))
            )
    else:
        probs = []
        for label in labels:
            inputs = [prompt.replace('[LABELS_CHOICE]', label) for prompt in prompts]
            probs.append(get_logprobs(model, tokenizer, inputs))
    return probs

if __name__ == '__main__':
    MODEL = sys.argv[1]
    prompt_lang = 'eng'
    BATCH_SIZE = 1
    SAVE_EVERY = 10

    out_dir = './outputs_nlu'
    metric_dir = './metrics_nlu'
    os.makedirs(out_dir, exist_ok=True) 
    os.makedirs(metric_dir, exist_ok=True) 

    # Load Prompt
    TASK_TYPE_TO_PROMPT = get_prompt(prompt_lang)

    # Load Dataset
    print('Load NLU Datasets...')
    nlu_datasets = load_nlu_datasets()
    #print("NLU: ", nlu_datasets)
    print(f'Loaded {len(nlu_datasets)} NLU datasets')
    for i, dset_subset in enumerate(nlu_datasets.keys()):
        print(f'{i} {dset_subset}')

    # Set seed before initializing model.
    set_seed(42)

    # Load Model
    tokenizer = AutoTokenizer.from_pretrained(MODEL, truncation_side='left', padding_side='right')
    if "bloom" in MODEL or "xglm" in MODEL or "gpt2" in MODEL or "Llama" in MODEL:
        model = AutoModelForCausalLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=False)
    else:
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=False)
        tokenizer.pad_token = tokenizer.eos_token # Use EOS to pad label
        
    model.eval()
    torch.no_grad()

    metrics = []
    labels = []
    for i, dset_subset in enumerate(nlu_datasets.keys()):
        print(f'{i} {dset_subset}')
        nlu_dset, task_type = nlu_datasets[dset_subset]
        if task_type.value not in TASK_TYPE_TO_PROMPT:
            print(f'SKIPPING {dset_subset}')
            continue

        # Retrieve metadata
        if dset_subset == 'haryoaw/COPAL' or dset_subset == 'IndoStoryCloze':
            split = 'test'
            if 'test' in nlu_dset.keys():
                test_dset = nlu_dset['test']
            else:
                test_dset = nlu_dset['train']
                split = 'train'

        elif dset_subset == 'cardiffnlp/tweet_topic_single':
            split = "test"
            if 'test_coling2022' in nlu_dset.keys():
                test_dset = nlu_dset['test_coling2022']               

        else:
            split = 'validation'
            if 'validation' in nlu_dset.keys():
                test_dset = nlu_dset['validation']
            else:
                test_dset = nlu_dset['train']
                split = 'train'

        # Retrieve & preprocess labels
        
        try:
            label_names = test_dset.features['label'].names
        except:
            label_names = list(set(test_dset['label']))
            
        # normalize some labels for more natural prompt:
        label_mapping = get_label_mapping(dset_subset, prompt_lang)
        label_names = sorted(list(map(lambda x: label_mapping[x], label_mapping))) # Why? This makes Try Except statement above useless

        label_to_id_dict = { l : i for i, l in enumerate(label_names)}
        
        dset_subset_std = dset_subset.replace("/","_")
        for prompt_id, prompt_template in enumerate(TASK_TYPE_TO_PROMPT[task_type.value]):
            inputs, preds, golds = [], [], []
            # Check saved data
            if exists(f'{out_dir}/{dset_subset_std}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv'):
                print("Output exist, use partial log instead")
                with open(f'{out_dir}/{dset_subset_std}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv') as csvfile:
                    reader = csv.DictReader(csvfile)
                    for row in reader:
                        inputs.append(row["Input"])
                        preds.append(row["Pred"])
                        golds.append(row["Gold"])
                print(f"Skipping until {len(preds)}")

            # sample prompt
            print("= LABEL NAME =")
            print(label_names)
            print("= SAMPLE PROMPT =")
            if 'COPAL' in dset_subset:
                print(to_prompt_copa(test_dset[0], prompt_template, label_names, prompt_lang))
            elif 'IndoStoryCloze' in dset_subset:
                print(to_prompt_indo_story_cloze(test_dset[0], prompt_template, label_names, prompt_lang))
            elif 'sst2' in dset_subset:
                print(to_prompt_sst2(test_dset[0], prompt_template, label_names, prompt_lang))
            elif 'qnli' in dset_subset:
                print(to_prompt_qnli(test_dset[0], prompt_template, label_names, prompt_lang))
            elif 'wnli' in dset_subset:
                print(to_prompt_wnli(test_dset[0], prompt_template, label_names, prompt_lang))
            elif 'tweet_topic_single' in dset_subset:
                print(to_prompt_tweet_topic_single(test_dset[0], prompt_template, label_names, prompt_lang))
                
            print("\n")            

            # zero-shot inference
            prompts, labels = [], []
            count = 0

            with torch.inference_mode():
                for e, sample in tqdm(enumerate(test_dset)):
                    if e < len(preds):
                        continue

                    # Add to buffer
                    if 'COPAL' in dset_subset:
                        prompt_text = to_prompt_copa(sample, prompt_template, label_names, prompt_lang)
                        label = int(sample['label'])
                    elif 'IndoStoryCloze' in dset_subset:
                        label_names = [sample['choice1'], sample['choice2']] # IndoStoryCloze label is dynamic
                        prompt_text = to_prompt_indo_story_cloze(sample, prompt_template, label_names, prompt_lang)
                        label = sample['label']
                    elif 'sst2' in dset_subset:
                        label_names = ['negative', 'positive'] # double check. Note: not dynamic
                        prompt_text = to_prompt_sst2(sample, prompt_template, label_names, prompt_lang)
                        label = sample['label']
                        
                    elif 'qnli' in dset_subset:
                        label_names = ['yes', 'no'] # double check
                        prompt_text = to_prompt_qnli(sample, prompt_template, label_names, prompt_lang)
                        label = sample['label']
                        
                    elif 'wnli' in dset_subset:
                        label_names = ['no', 'yes'] # double check
                        prompt_text = to_prompt_wnli(sample, prompt_template, label_names, prompt_lang)
                        label = sample['label']
                        
                    elif 'tweet_topic_single' in dset_subset:
                        #label_names = [sample['label_name']] # double check
                        label_names = test_dset.features['label'].names
                        label_names = [item.replace('_', ' ') for item in label_names]
                        prompt_text = to_prompt_tweet_topic_single(sample, prompt_template, label_names, prompt_lang)
                        label = sample['label']
                    
                    prompts.append(prompt_text)
                    labels.append(label)

                    # Batch Inference
                    if len(prompts) == BATCH_SIZE:                        
                        out = predict_classification(model, tokenizer, prompts, label_names)
                        hyps = argmax(stack(out, axis=-1), axis=-1).tolist()
                        for (prompt_text, hyp, label) in zip(prompts, hyps, labels):
                            inputs.append(prompt_text)
                            preds.append(hyp)
                            golds.append(label)
                        prompts, labels = [], []
                        count += 1
                        
                    if count == SAVE_EVERY:
                        # partial saving
                        inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
                        inference_df.to_csv(f'{out_dir}/{dset_subset_std}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv', index=False)
                        count = 0
                        
                if len(prompts) > 0:
                    out = predict_classification(model, tokenizer, prompts, label_names)
                    hyps = argmax(stack(out, axis=-1), axis=-1).tolist()
                    for (prompt_text, hyp, label) in zip(prompts, hyps, labels):
                        inputs.append(prompt_text)
                        preds.append(hyp)
                        golds.append(label)
                    prompts, labels = [], []

            # partial saving
            inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
            inference_df.to_csv(f'{out_dir}/{dset_subset_std}_{prompt_lang}_{prompt_id}_{MODEL.split("/")[-1]}.csv', index=False)

            cls_report = classification_report(golds, preds, output_dict=True)
            micro_f1, micro_prec, micro_rec, _ = precision_recall_fscore_support(golds, preds, average='micro')
            print(dset_subset)
            print('accuracy', cls_report['accuracy'])
            print('f1 micro', micro_f1)
            print('f1 macro', cls_report['macro avg']['f1-score'])
            print('f1 weighted', cls_report['weighted avg']['f1-score'])
            print("===\n\n")       

            metrics.append({
                'dataset': dset_subset,
                'prompt_id': prompt_id,
                'prompt_lang': prompt_lang,
                'accuracy': cls_report['accuracy'], 
                'micro_prec': micro_prec,
                'micro_rec': micro_rec,
                'micro_f1_score': micro_f1,
                'macro_prec': cls_report['macro avg']['precision'],
                'macro_rec': cls_report['macro avg']['recall'],
                'macro_f1_score': cls_report['macro avg']['f1-score'],
                'weighted_prec': cls_report['weighted avg']['precision'],
                'weighted_rec': cls_report['weighted avg']['recall'],
                'weighted_f1_score': cls_report['weighted avg']['f1-score'],
            })

    pd.DataFrame(metrics).reset_index().to_csv(f'{metric_dir}/results_{prompt_lang}_{MODEL.split("/")[-1]}.csv', index=False)
