{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab79154b-8ec4-43fe-819c-4f5c2bfcccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sentiment_analysis import *\n",
    "test_examples, test_ids = nusax_sentiment()\n",
    "\n",
    "from collections import defaultdict\n",
    "# get_gpt_generation() function from utils\n",
    "from utils import * \n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9277962-439f-4aae-b83b-18203e87b628",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902226f-0385-4f22-a486-745a6d080492",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b13283-e799-4831-b966-e8e67056b57a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis():\n",
    "    answers = defaultdict(list)\n",
    "    do_print = True\n",
    "    \n",
    "    for ex in test_examples:\n",
    "        prompt = f\"\\\"{ex['text']}\\\". What would be the sentiment of the text? positive, negative, or neutral?\"\n",
    "    \n",
    "        ###### HERE YOU CAN CHANGE IT TO YOUR OWN MODELS########\n",
    "        gen = get_gpt_generation(prompt)\n",
    "        ########################################################\n",
    "    \n",
    "        # Here, you cannot fully depedent on parsed answer --> highly recommend double check\n",
    "        gen_split = gen.lower().split()\n",
    "    \n",
    "        a = -1\n",
    "        for x in gen_split:\n",
    "            if 'positive' in x:\n",
    "                a = 'positive'\n",
    "            elif 'negative' in x:\n",
    "                a = 'negative'\n",
    "            elif 'neutral' in x:\n",
    "                a = 'neutral'\n",
    "                \n",
    "            if a != -1:\n",
    "                break\n",
    "                \n",
    "        if a == -1:\n",
    "            a = 'x' \n",
    "\n",
    "        parsed_answer = a\n",
    "            \n",
    "        ex['gen'] = gen\n",
    "        ex['parsed_answer'] = parsed_answer\n",
    "        answers[ex['lang']].append({'label': ex['label'], 'a' : parsed_answer, 'gen': gen})\n",
    "        \n",
    "        with jsonlines.open('results/sentiment_analysis.jsonl', mode='a') as writer:\n",
    "            writer.write(ex)\n",
    "    \n",
    "        if do_print:\n",
    "            print(f\"{ex['lang']})) {ex['label']} | {parsed_answer} | {gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582bebf-28ca-4118-833d-5f576bd0cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d5f881-a0b8-409e-901e-8fddfa858d16",
   "metadata": {},
   "source": [
    "## Scoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d47dbb66-44f8-4d9d-8c8f-673d0e66b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs= []\n",
    "\n",
    "with jsonlines.open('results/sentiment_analysis.jsonl') as read_file:\n",
    "    for line in read_file.iter():\n",
    "    \tobjs.append(line)\n",
    "\n",
    "do_print = False\n",
    "\n",
    "counter = defaultdict(list)\n",
    "for o in objs:\n",
    "    a = o['a']\n",
    "\n",
    "    correct = 1 if a == o['label'] else 0\n",
    "    counter[o['lang']].append(correct)\n",
    "    \n",
    "    if do_print:\n",
    "        print(f\"{o['lang']} | {o['label']} | {a} | {gen.strip()} | {correct}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c77ac0c-e707-43c6-8ea6-fc26b99bfa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buginese :  0.08\n",
      "English :  0.82\n",
      "Indonesian :  0.76\n",
      "Javanese :  0.6\n"
     ]
    }
   ],
   "source": [
    "# # PRINT\n",
    "for k, v in counter.items():\n",
    "    print(k,\": \", sum(v)/len(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccfe189-4860-443c-a5b3-08556ea01c5b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# LID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535baad-b2df-4feb-b492-377411e42d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3369bf31-6b8f-4f83-aad3-727b359c4ffb",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee920218-f93b-429e-b011-d31ca6a34d93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lid():\n",
    "    df = pd.read_csv(\"data/LID.csv\")\n",
    "    for idx, row in df.iterrows():\n",
    "        prompt = row['Text']\n",
    "        label = row['label']\n",
    "        \n",
    "    ###### HERE YOU CAN CHANGE IT TO YOUR OWN MODELS########\n",
    "        gen = get_gpt_generation(prompt)\n",
    "    ########################################################\n",
    "        \n",
    "        if label in gen.lower():\n",
    "            a = 1\n",
    "        else:\n",
    "            a = 0\n",
    "    \n",
    "        obj = {\n",
    "            'prompt' : prompt,\n",
    "            'label' : label, \n",
    "            'gen' : gen,\n",
    "            'correct' : a\n",
    "        }\n",
    "        with jsonlines.open('lid.jsonl', mode='a') as writer:\n",
    "            writer.write(obj) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb2335-29fb-4692-8115-19dfe5b11456",
   "metadata": {},
   "outputs": [],
   "source": [
    "lid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c492b323-acd0-4c1f-bbed-098fd2c13ae6",
   "metadata": {},
   "source": [
    "## Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f687234e-9179-488c-bb5d-e20e799fa6c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indonesian :  1.0\n",
      "english :  0.92\n",
      "javanese :  0.9\n",
      "buginese :  0.64\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "lid_objs= []\n",
    "with jsonlines.open('results/lid.jsonl') as read_file:\n",
    "    for line in read_file.iter():\n",
    "    \tlid_objs.append(line)\n",
    "\n",
    "# COUNT\n",
    "do_print = False\n",
    "counter = defaultdict(list)\n",
    "for o in lid_objs:\n",
    "    counter[o['label']].append(o['correct'])\n",
    "\n",
    "    if do_print:\n",
    "        if o['correct'] == 0:\n",
    "            print(o['label'], \"|| \", o['gen'], o['prompt'])\n",
    "\n",
    "# PRINT\n",
    "for k, v in counter.items():\n",
    "    print(k,\": \", sum(v)/len(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7c1bc-1a82-4cd4-999d-d4a4003c73ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b8809e-7c67-410e-8c34-d96810c417ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a0aa65-fa7f-4128-8726-ebd73fd5cd96",
   "metadata": {},
   "source": [
    "## SA - F1 SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f81be4b6-bdc4-46b6-bd5a-c4493225f8d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955608834641092"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "df = pd.read_csv(\"sa.csv\")\n",
    "\n",
    "result_lang_label = defaultdict(list)\n",
    "result_lang_gen = defaultdict(list)\n",
    "for idx, row in df.iterrows():\n",
    "    result_lang_label[row['lang']].append(row['label'])\n",
    "    result_lang_gen[row['lang']].append(row['gen'])\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "lang = 'Indonesian'\n",
    "f1_score(result_lang_label[lang], result_lang_gen[lang], average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44558ee-cc4f-4f2d-af14-7526ade2730f",
   "metadata": {},
   "source": [
    "buginese 0.12373737373737373\n",
    "\n",
    "English 0.8121693121693121\n",
    "\n",
    "JAVANESE 0.6167613636363637\n",
    "\n",
    "Indonesian 0.8955608834641092"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e911a-7834-44ff-8a4f-380b063a16d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
