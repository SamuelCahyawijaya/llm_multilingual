"""nusacrowd zero-shot prompt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ru8DyS2ALWfRdkjOPHj-KNjw6Pfa44Nd
"""
import os, sys
import csv
from os.path import exists

from numpy import argmax, stack
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.metrics import classification_report, precision_recall_fscore_support

import torch
import torch.nn.functional as F

#from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, set_seed

from prompt_utils import get_prompt, get_label_mapping
from data_utils import load_chatgpt_eval_tasks
from peft import PeftModel

#!pip install git+https://github.com/IndoNLP/nusa-crowd.git@release_exp
#!pip install transformers
#!pip install sentencepiece

DEBUG=False

def to_prompt_chatgpt_eval(input):
    prompt = f'{input["question"]}\nAnswer: [LABELS_CHOICE]'
    return prompt

@torch.inference_mode()
def get_logprobs(model, tokenizer, inputs, label_ids=None, label_attn=None):
    inputs = tokenizer(inputs, return_tensors="pt", padding=True, truncation=True, max_length=1024).to('cuda')    
    if model.config.is_encoder_decoder:
        label_ids = label_ids.repeat((inputs['input_ids'].shape[0],1))
        label_attn = label_attn.repeat((inputs['input_ids'].shape[0],1))
        logits = model(**inputs, labels=label_ids).logits
        logprobs = torch.gather(F.log_softmax(logits, dim=-1), 2, label_ids.unsqueeze(2)).squeeze(dim=-1) * label_attn # Info for decoder to generate label token(s)
        return logprobs.sum(dim=-1).cpu()
    else:
        logits = model(**inputs).logits
        output_ids = inputs["input_ids"][:, 1:]
        logprobs = torch.gather(F.log_softmax(logits, dim=-1), 2, output_ids.unsqueeze(2)).squeeze(dim=-1)
        logprobs[inputs["attention_mask"][:, :-1] == 0] = 0
        return logprobs.sum(dim=1).cpu()


@torch.inference_mode()
def predict_classification(model, tokenizer, prompts, labels):
    if model.config.is_encoder_decoder:
        labels_encoded = tokenizer(labels, add_special_tokens=False, padding=True, return_tensors='pt')
        list_label_ids = labels_encoded['input_ids'].to('cuda')
        list_label_attn = labels_encoded['attention_mask'].to('cuda')
        
        inputs = [prompt.replace('[LABELS_CHOICE]', '') for prompt in prompts]
        probs = []
        for (label_ids, label_attn) in zip(list_label_ids, list_label_attn):
            probs.append(
                get_logprobs(model, tokenizer, inputs, label_ids.view(1,-1), label_attn.view(1,-1))
            )
    else:
        probs = []
        for label in labels:
            inputs = [prompt.replace('[LABELS_CHOICE]', label) for prompt in prompts]
            probs.append(get_logprobs(model, tokenizer, inputs))
    return probs

if __name__ == '__main__':
    MODEL = sys.argv[1]
    BATCH_SIZE = 1
    SAVE_EVERY = 10

    out_dir = './outputs_chatgpt_eval'
    metric_dir = './metrics_chatgpt_eval'
    os.makedirs(out_dir, exist_ok=True) 
    os.makedirs(metric_dir, exist_ok=True) 

    # Load Dataset
    print('Load ChatGPT Evaluation Datasets...')
    nlu_datasets = load_chatgpt_eval_tasks()
    #print("NLU: ", nlu_datasets)
    print(f'Loaded {len(nlu_datasets)} NLU datasets')
    for i, dset_subset in enumerate(nlu_datasets.keys()):
        print(f'{i} {dset_subset}')

    # Set seed before initializing model.
    set_seed(42)

    # Load Model
    if MODEL in ['tiiuae/falcon-7b', 'tiiuae/falcon-40b']:
        tokenizer = AutoTokenizer.from_pretrained(MODEL, truncation_side='right', padding_side='right')
        tokenizer.pad_token = tokenizer.eos_token
        model = AutoModelForCausalLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=True)
        ADAPTER = f'{MODEL}/baseline'
    else:
        ADAPTER = MODEL
        if '7b' in ADAPTER:
            MODEL = 'tiiuae/falcon-7b'
        else:
            MODEL = 'tiiuae/falcon-40b'
            
        tokenizer = AutoTokenizer.from_pretrained(MODEL, truncation_side='left', padding_side='left')
        tokenizer.pad_token = tokenizer.eos_token
        model = AutoModelForCausalLM.from_pretrained(MODEL, device_map="auto", load_in_8bit=True)
        model = PeftModel.from_pretrained(model, ADAPTER)
        model = model.merge_and_unload()
    
    model.eval()
    torch.no_grad()

    metrics = []
    labels = []
    for i, dset_subset in enumerate(nlu_datasets.keys()):
        print(f'{i} {dset_subset}')
        test_dset = nlu_datasets[dset_subset]
        
        inputs, preds, golds = [], [], []
        # Check saved data
        if exists(f'{out_dir}/{dset_subset}_{ADAPTER.split("/")[-2]}.csv'):
            print("Output exist, use partial log instead")
            with open(f'{out_dir}/{dset_subset}_{ADAPTER.split("/")[-2]}.csv') as csvfile:
                reader = csv.DictReader(csvfile)
                for row in reader:
                    inputs.append(row["Input"])
                    preds.append(row["Pred"])
                    golds.append(row["Gold"])
            print(f"Skipping until {len(preds)}")

        # sample prompt
        print("= SAMPLE PROMPT =")
        print(to_prompt_chatgpt_eval(test_dset[0]))
        print("\n")            

        # zero-shot inference
        prompts, labels = [], []
        count = 0

        with torch.inference_mode():
            for e, sample in tqdm(enumerate(test_dset)):
                if e < len(preds):
                    continue

                # Add to buffer
                label_names = sample['choice']
                prompt_text = to_prompt_chatgpt_eval(sample)
                label = sample['label']

                prompts.append(prompt_text)
                labels.append(label)

                # Batch Inference
                if len(prompts) == BATCH_SIZE:                        
                    out = predict_classification(model, tokenizer, prompts, label_names)
                    hyps = argmax(stack(out, axis=-1), axis=-1).tolist()
                    for (prompt_text, hyp, label) in zip(prompts, hyps, labels):
                        inputs.append(prompt_text)
                        preds.append(label_names[hyp])
                        golds.append(label)
                    prompts, labels = [], []
                    count += 1

                if count == SAVE_EVERY:
                    # partial saving
                    inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
                    inference_df.to_csv(f'{out_dir}/{dset_subset}_{ADAPTER.split("/")[-2]}.csv', index=False)
                    count = 0

            if len(prompts) > 0:
                out = predict_classification(model, tokenizer, prompts, label_names)
                hyps = argmax(stack(out, axis=-1), axis=-1).tolist()
                for (prompt_text, hyp, label) in zip(prompts, hyps, labels):
                    inputs.append(prompt_text)
                    preds.append(label_names[hyp])
                    golds.append(label)
                prompts, labels = [], []

            # partial saving
            inference_df = pd.DataFrame(list(zip(inputs, preds, golds)), columns =["Input", 'Pred', 'Gold'])
            inference_df.to_csv(f'{out_dir}/{dset_subset}_{ADAPTER.split("/")[-2]}.csv', index=False)

            correct = 0
            for g, p in zip(golds, preds):
                if type(g) == list:
                    correct += 1 if p in g else 0
                else:
                    correct += 1 if p == g else 0
            accuracy = correct / len(golds)
            print(dset_subset)
            print('accuracy', accuracy)
            print("===\n\n")       

            metrics.append({
                'dataset': dset_subset,
                'prompt_id': 0,
                'prompt_lang': 'eng',
                'accuracy': accuracy, 
                'micro_prec': np.nan,
                'micro_rec': np.nan,
                'micro_f1_score': np.nan,
                'macro_prec': np.nan,
                'macro_rec': np.nan,
                'macro_f1_score': np.nan,
                'weighted_prec': np.nan,
                'weighted_rec': np.nan,
                'weighted_f1_score': np.nan,
            })

    pd.DataFrame(metrics).reset_index().to_csv(f'{metric_dir}/results_{ADAPTER.split("/")[-2]}.csv', index=False)
